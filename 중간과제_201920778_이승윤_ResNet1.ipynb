{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8jd6Q4yDNxlAnrLHwmLf0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soccer1356-2000/Ingong_MidtermExam/blob/main/%EC%A4%91%EA%B0%84%EA%B3%BC%EC%A0%9C_201920778_%EC%9D%B4%EC%8A%B9%EC%9C%A4_ResNet1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args={}\n",
        "kwargs={}\n",
        "args['batch_size']=32\n",
        "args['epochs']=10  #The number of Epochs is the number of times you go through the full dataset. \n",
        "args['lr']=0.01 #Learning rate is how fast it will decend. \n",
        "args['momentum']=0.9 #SGD momentum (default: 0.5) Momentum is a moving average of our gradients (helps to keep direction).\n",
        "args['weight_decay']=1e-5\n",
        "\n",
        "args['seed']=1 #random seed\n",
        "args['log_interval']=5000 // args['batch_size']\n",
        "args['cuda']=True\n"
      ],
      "metadata": {
        "id": "QrIE9UAm0d0b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FoWYQ5rfLSLH"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms # Fashion-MNIST dataset for PyTorch\n",
        "# Download and load the FashionMNIST training data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True,\n",
        "transform = transform)\n",
        "testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False,\n",
        "transform = transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=args['batch_size'], shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=args['batch_size'], shuffle=True)"
      ],
      "metadata": {
        "id": "FbEBR-PBNQbC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset # 28 * 28 * 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZcKmEcXa6BP",
        "outputId": "c3dbd456-802e-40b2-b8ff-862649272fea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset FashionMNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: MNIST_data/\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5,), std=(0.5,))\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "6yjaet1MoX-Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, groups=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, groups=groups, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
      ],
      "metadata": {
        "id": "-lSZGD5wndIa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64,  norm_layer=nn.BatchNorm2d):\n",
        "      \n",
        "        super().__init__()\n",
        "            \n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)  \n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "            \n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "BKX_QBynryp8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, norm_layer=nn.BatchNorm2d):\n",
        "        super().__init__()\n",
        "\n",
        "        # ResNext\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        \n",
        "\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups) # conv2에서 downsample\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        # 1x1 convolution layer\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        # 3x3 convolution layer\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        # 1x1 convolution layer\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        \n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "CimQEbX0sfJN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, layers, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, norm_layer=nn.BatchNorm2d):\n",
        "    super().__init__()\n",
        "    self.norm_layer = norm_layer\n",
        "    self.groups = groups\n",
        "    self.base_width = width_per_group\n",
        "\n",
        "    self.inplane = 64\n",
        "\n",
        "    #stage 1\n",
        "    self.conv1 = nn.Conv2d(1,self.inplane,kernel_size=7,stride = 2, padding = 3, bias = False)\n",
        "    self.bn1 = norm_layer(self.inplane)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "    #stage 2\n",
        "    self.layer1 = self.make_layer(block,64,layers[0])\n",
        "    #stage 3\n",
        "    self.layer2 = self.make_layer(block,128,layers[1],stride=2)\n",
        "    #stage 4\n",
        "    self.layer3 = self.make_layer(block,256,layers[2],stride=2)\n",
        "    #stage 5\n",
        "    self.layer4 = self.make_layer(block,512,layers[3],stride=2)\n",
        "    self.averpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(512 * block.expansion,10)\n",
        "  \n",
        "  def make_layer(self,block,palnes,blocks,stride = 1):\n",
        "    norm_layer = self.norm_layer\n",
        "\n",
        "    down_channel = None\n",
        "    \n",
        "    if stride != 1 or self.inplane != block.expansion * palnes:\n",
        "      down_channel = nn.Sequential(conv1x1(self.inplane, palnes * block.expansion,stride),norm_layer(palnes * block.expansion))\n",
        "\n",
        "    layers = []\n",
        "\n",
        "    layers.append(block(self.inplane, palnes, stride, down_channel, self.groups,self.base_width, norm_layer))\n",
        "    self.inplane = palnes * block.expansion # inplanes 업데이트\n",
        "    for _ in range(1, blocks):\n",
        "        layers.append(block(self.inplane, palnes, groups=self.groups,base_width=self.base_width, norm_layer=norm_layer))\n",
        "        self.inplane = palnes * block.expansion # inplanes 업데이트\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "\n",
        "    x = self.averpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "   # output = nn.log_softmax(x, dim=1)\n",
        "    return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "yjZKnwgct0hB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "G_UN2T5709Rs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "        if args['cuda']:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        #Variables in Pytorch are differenciable. \n",
        "        data, target = Variable(data), Variable(target)\n",
        "        #This will zero out the gradients for this batch. \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        train_pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        train_correct += train_pred.eq(target.data.view_as(train_pred)).long().cpu().sum()\n",
        "        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\n",
        "        loss = F.nll_loss(output, target)\n",
        "        #dloss/dx for every Variable \n",
        "        loss.backward()\n",
        "        #to do a one-step update on our parameter.\n",
        "        optimizer.step()\n",
        "        #Print out the loss periodically. \n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            train_correct = 0\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
        "                100. * batch_idx / len(trainloader), loss.data))\n",
        "    train_acc.insert(len(train_acc),(100. * train_correct / len(trainloader.dataset)))\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    correct_top5 = 0\n",
        "    total = 0\n",
        "    total_r=0\n",
        "    correct_top1=0\n",
        "    for data, target in testloader:\n",
        "        if args['cuda']:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
        "\n",
        "        _, predicted = torch.topk(output, k=5, dim=1)\n",
        "        total += target.size(0)\n",
        "        correct_top5 += sum([1 if label in predicted[i] else 0  for i,label in enumerate(target)])\n",
        "\n",
        "        __, predicted_r = torch.max(output, 1)\n",
        "        total_r += target.size(0)\n",
        "        correct_top1 += (predicted_r == target).sum().item()\n",
        "\n",
        "    test_loss /= len(testloader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(testloader.dataset),\n",
        "        100. * correct / len(testloader.dataset)))\n",
        "    test_acc.insert(len(test_acc),(100. * correct / len(testloader.dataset)))\n",
        "    top5_accuracy = correct_top5 / total\n",
        "    print(f\"Top-5 Accuracy: {top5_accuracy}\")\n",
        "    top1_accuracy = correct_top1 / total_r\n",
        "    print(f\"Top-1 Accuracy: {top1_accuracy}\\n\")\n",
        "  "
      ],
      "metadata": {
        "id": "URFhhVfUutlU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ptflops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9iyKkvXDkUc",
        "outputId": "b40bee12-d5df-428c-9ef1-34b4b3f1c139"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->ptflops) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->ptflops) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ptflops) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ptflops) (1.3.0)\n",
            "Building wheels for collected packages: ptflops\n",
            "  Building wheel for ptflops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ptflops: filename=ptflops-0.7-py3-none-any.whl size=11076 sha256=43dcceae34cadf637834026e2180774584edde3d5ac0db3b8bc46d261870cde2\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/54/3b/f84523431ce82e08462644d279c0e13a51a00236e237e6bc7e\n",
            "Successfully built ptflops\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from ptflops import get_model_complexity_info\n",
        "import math\n",
        "import time"
      ],
      "metadata": {
        "id": "6m_HDnysAp5c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "model = ResNet(Bottleneck,[3,4,6,3])\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'],weight_decay=args['weight_decay'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(f\"{end - start:.5f} sec\")\n",
        "\n",
        "with torch.cuda.device(0):\n",
        "\n",
        "  input_tensor = torch.randn(1, 1, 224, 224).cuda()\n",
        "\n",
        "  flops, params = get_model_complexity_info(model, (1, 224, 224), as_strings=True, print_per_layer_stat=True)\n",
        "\n",
        "  print('FLOPs:', flops)\n",
        "\n",
        "  print('Parameters:', params)\n",
        "\n",
        "\n",
        "print(test_acc)\n",
        "print(train_acc)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot([i for i in range(1,len(test_acc)+1)], test_acc, color =\"blue\")\n",
        "a = plt.twinx()\n",
        "a.plot([i for i in range(1,len(train_acc)+1)], train_acc, color = \"red\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j6MJRqJ20YDM",
        "outputId": "9b43de4f-abe4-4f09-f18f-6d154e52d29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-36c03540b093>:60: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.571348\n",
            "Train Epoch: 1 [4992/60000 (8%)]\tLoss: 2.400527\n",
            "Train Epoch: 1 [9984/60000 (17%)]\tLoss: 1.483584\n",
            "Train Epoch: 1 [14976/60000 (25%)]\tLoss: 1.572399\n",
            "Train Epoch: 1 [19968/60000 (33%)]\tLoss: 2.191091\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.942178\n",
            "Train Epoch: 1 [29952/60000 (50%)]\tLoss: 3.502592\n",
            "Train Epoch: 1 [34944/60000 (58%)]\tLoss: 2.538315\n",
            "Train Epoch: 1 [39936/60000 (67%)]\tLoss: 5.870281\n",
            "Train Epoch: 1 [44928/60000 (75%)]\tLoss: 1.016972\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.162630\n",
            "Train Epoch: 1 [54912/60000 (92%)]\tLoss: 0.580461\n",
            "Train Epoch: 1 [59904/60000 (100%)]\tLoss: 1.195467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-d3b87ec97dcd>:38: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.0329, Accuracy: 6250/10000 (62%)\n",
            "Top-5 Accuracy: 0.9835\n",
            "Top-1 Accuracy: 0.625\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.775056\n",
            "Train Epoch: 2 [4992/60000 (8%)]\tLoss: 1.807752\n",
            "Train Epoch: 2 [9984/60000 (17%)]\tLoss: 0.905820\n",
            "Train Epoch: 2 [14976/60000 (25%)]\tLoss: 2.804996\n",
            "Train Epoch: 2 [19968/60000 (33%)]\tLoss: 0.983391\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 4.796560\n",
            "Train Epoch: 2 [29952/60000 (50%)]\tLoss: 0.923590\n",
            "Train Epoch: 2 [34944/60000 (58%)]\tLoss: 4.763465\n",
            "Train Epoch: 2 [39936/60000 (67%)]\tLoss: 0.700604\n",
            "Train Epoch: 2 [44928/60000 (75%)]\tLoss: 0.634912\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.648374\n",
            "Train Epoch: 2 [54912/60000 (92%)]\tLoss: 0.420335\n",
            "Train Epoch: 2 [59904/60000 (100%)]\tLoss: 0.286328\n",
            "\n",
            "Test set: Average loss: 0.6659, Accuracy: 7757/10000 (78%)\n",
            "Top-5 Accuracy: 0.9956\n",
            "Top-1 Accuracy: 0.7757\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.597273\n",
            "Train Epoch: 3 [4992/60000 (8%)]\tLoss: 0.757632\n",
            "Train Epoch: 3 [9984/60000 (17%)]\tLoss: 0.641658\n",
            "Train Epoch: 3 [14976/60000 (25%)]\tLoss: 0.561738\n",
            "Train Epoch: 3 [19968/60000 (33%)]\tLoss: 0.424694\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.499560\n",
            "Train Epoch: 3 [29952/60000 (50%)]\tLoss: 0.794552\n",
            "Train Epoch: 3 [34944/60000 (58%)]\tLoss: 0.450184\n",
            "Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.658476\n",
            "Train Epoch: 3 [44928/60000 (75%)]\tLoss: 0.651504\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.893505\n",
            "Train Epoch: 3 [54912/60000 (92%)]\tLoss: 0.616020\n",
            "Train Epoch: 3 [59904/60000 (100%)]\tLoss: 1.048360\n",
            "\n",
            "Test set: Average loss: 0.6040, Accuracy: 7761/10000 (78%)\n",
            "Top-5 Accuracy: 0.9958\n",
            "Top-1 Accuracy: 0.7761\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.448334\n",
            "Train Epoch: 4 [4992/60000 (8%)]\tLoss: 0.434130\n",
            "Train Epoch: 4 [9984/60000 (17%)]\tLoss: 0.416754\n",
            "Train Epoch: 4 [14976/60000 (25%)]\tLoss: 0.256860\n",
            "Train Epoch: 4 [19968/60000 (33%)]\tLoss: 0.557734\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.638322\n",
            "Train Epoch: 4 [29952/60000 (50%)]\tLoss: 0.582957\n",
            "Train Epoch: 4 [34944/60000 (58%)]\tLoss: 0.423643\n",
            "Train Epoch: 4 [39936/60000 (67%)]\tLoss: 0.643164\n",
            "Train Epoch: 4 [44928/60000 (75%)]\tLoss: 0.448949\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 1.119875\n",
            "Train Epoch: 4 [54912/60000 (92%)]\tLoss: 0.533672\n",
            "Train Epoch: 4 [59904/60000 (100%)]\tLoss: 0.476160\n",
            "\n",
            "Test set: Average loss: 0.5069, Accuracy: 8168/10000 (82%)\n",
            "Top-5 Accuracy: 0.9966\n",
            "Top-1 Accuracy: 0.8168\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.341665\n",
            "Train Epoch: 5 [4992/60000 (8%)]\tLoss: 0.340704\n",
            "Train Epoch: 5 [9984/60000 (17%)]\tLoss: 0.939110\n",
            "Train Epoch: 5 [14976/60000 (25%)]\tLoss: 0.505304\n",
            "Train Epoch: 5 [19968/60000 (33%)]\tLoss: 0.843718\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.478253\n",
            "Train Epoch: 5 [29952/60000 (50%)]\tLoss: 0.428277\n",
            "Train Epoch: 5 [34944/60000 (58%)]\tLoss: 0.325456\n",
            "Train Epoch: 5 [39936/60000 (67%)]\tLoss: 0.283718\n",
            "Train Epoch: 5 [44928/60000 (75%)]\tLoss: 0.490395\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.578705\n",
            "Train Epoch: 5 [54912/60000 (92%)]\tLoss: 0.659900\n",
            "Train Epoch: 5 [59904/60000 (100%)]\tLoss: 0.454244\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-bb116d84d866>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-d3b87ec97dcd>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;31m# sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-36c03540b093>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-4af4fe9e3953>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# 1x1 convolution layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ResNeXt\n",
        "torch.cuda.empty_cache()\n",
        "start = time.time()\n",
        "model = ResNet(Bottleneck,[3,4,6,3],groups=32)\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'],weight_decay=args['weight_decay'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "end = time.time()\n",
        "print(f\"{end - start:.5f} sec\")\n",
        "\n",
        "with torch.cuda.device(0):\n",
        "\n",
        "  input_tensor = torch.randn(1, 1, 224, 224).cuda()\n",
        "\n",
        "  flops, params = get_model_complexity_info(model, (1, 224, 224), as_strings=True, print_per_layer_stat=True)\n",
        "\n",
        "  print('FLOPs:', flops)\n",
        "\n",
        "  print('Parameters:', params)\n",
        "\n",
        "\n",
        "print(test_acc)\n",
        "print(train_acc)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot([i for i in range(1,len(test_acc)+1)], test_acc, color =\"blue\")\n",
        "a = plt.twinx()\n",
        "a.plot([i for i in range(1,len(train_acc)+1)], train_acc, color = \"red\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IL_sKr34slks",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2b25905-f536-4596-cbd9-7bf72ca07caa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-36c03540b093>:60: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.707015\n",
            "Train Epoch: 1 [4992/60000 (8%)]\tLoss: 1.704679\n",
            "Train Epoch: 1 [9984/60000 (17%)]\tLoss: 0.821777\n",
            "Train Epoch: 1 [14976/60000 (25%)]\tLoss: 1.378377\n",
            "Train Epoch: 1 [19968/60000 (33%)]\tLoss: 4.143837\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.519479\n",
            "Train Epoch: 1 [29952/60000 (50%)]\tLoss: 0.704641\n",
            "Train Epoch: 1 [34944/60000 (58%)]\tLoss: 0.607279\n",
            "Train Epoch: 1 [39936/60000 (67%)]\tLoss: 1.296994\n",
            "Train Epoch: 1 [44928/60000 (75%)]\tLoss: 0.625946\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.548169\n",
            "Train Epoch: 1 [54912/60000 (92%)]\tLoss: 1.012455\n",
            "Train Epoch: 1 [59904/60000 (100%)]\tLoss: 0.309120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-d3b87ec97dcd>:38: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6072, Accuracy: 7902/10000 (79%)\n",
            "Top-5 Accuracy: 0.9944\n",
            "Top-1 Accuracy: 0.7902\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.552678\n",
            "Train Epoch: 2 [4992/60000 (8%)]\tLoss: 0.390429\n",
            "Train Epoch: 2 [9984/60000 (17%)]\tLoss: 0.648274\n",
            "Train Epoch: 2 [14976/60000 (25%)]\tLoss: 0.308045\n",
            "Train Epoch: 2 [19968/60000 (33%)]\tLoss: 0.379134\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.964928\n",
            "Train Epoch: 2 [29952/60000 (50%)]\tLoss: 0.438013\n",
            "Train Epoch: 2 [34944/60000 (58%)]\tLoss: 0.522075\n",
            "Train Epoch: 2 [39936/60000 (67%)]\tLoss: 0.523011\n",
            "Train Epoch: 2 [44928/60000 (75%)]\tLoss: 0.469957\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.499711\n",
            "Train Epoch: 2 [54912/60000 (92%)]\tLoss: 0.369099\n",
            "Train Epoch: 2 [59904/60000 (100%)]\tLoss: 0.381003\n",
            "\n",
            "Test set: Average loss: 0.8420, Accuracy: 8268/10000 (83%)\n",
            "Top-5 Accuracy: 0.9951\n",
            "Top-1 Accuracy: 0.8268\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.331210\n",
            "Train Epoch: 3 [4992/60000 (8%)]\tLoss: 0.419902\n",
            "Train Epoch: 3 [9984/60000 (17%)]\tLoss: 0.570699\n",
            "Train Epoch: 3 [14976/60000 (25%)]\tLoss: 0.662960\n",
            "Train Epoch: 3 [19968/60000 (33%)]\tLoss: 0.581377\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.474685\n",
            "Train Epoch: 3 [29952/60000 (50%)]\tLoss: 0.478044\n",
            "Train Epoch: 3 [34944/60000 (58%)]\tLoss: 0.580401\n",
            "Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.558271\n",
            "Train Epoch: 3 [44928/60000 (75%)]\tLoss: 0.650390\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.326335\n",
            "Train Epoch: 3 [54912/60000 (92%)]\tLoss: 0.570831\n",
            "Train Epoch: 3 [59904/60000 (100%)]\tLoss: 0.267920\n",
            "\n",
            "Test set: Average loss: 1.0073, Accuracy: 8274/10000 (83%)\n",
            "Top-5 Accuracy: 0.9936\n",
            "Top-1 Accuracy: 0.8274\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.368445\n",
            "Train Epoch: 4 [4992/60000 (8%)]\tLoss: 0.324899\n",
            "Train Epoch: 4 [9984/60000 (17%)]\tLoss: 0.419032\n",
            "Train Epoch: 4 [14976/60000 (25%)]\tLoss: 0.534316\n",
            "Train Epoch: 4 [19968/60000 (33%)]\tLoss: 0.245727\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.246954\n",
            "Train Epoch: 4 [29952/60000 (50%)]\tLoss: 0.570528\n",
            "Train Epoch: 4 [34944/60000 (58%)]\tLoss: 0.323271\n",
            "Train Epoch: 4 [39936/60000 (67%)]\tLoss: 0.573539\n",
            "Train Epoch: 4 [44928/60000 (75%)]\tLoss: 0.439004\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.305741\n",
            "Train Epoch: 4 [54912/60000 (92%)]\tLoss: 0.303560\n",
            "Train Epoch: 4 [59904/60000 (100%)]\tLoss: 0.615657\n",
            "\n",
            "Test set: Average loss: 0.9072, Accuracy: 8280/10000 (83%)\n",
            "Top-5 Accuracy: 0.9962\n",
            "Top-1 Accuracy: 0.828\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.188458\n",
            "Train Epoch: 5 [4992/60000 (8%)]\tLoss: 0.576986\n",
            "Train Epoch: 5 [9984/60000 (17%)]\tLoss: 0.118210\n",
            "Train Epoch: 5 [14976/60000 (25%)]\tLoss: 0.179111\n",
            "Train Epoch: 5 [19968/60000 (33%)]\tLoss: 0.474703\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.357318\n",
            "Train Epoch: 5 [29952/60000 (50%)]\tLoss: 0.214579\n",
            "Train Epoch: 5 [34944/60000 (58%)]\tLoss: 0.423728\n",
            "Train Epoch: 5 [39936/60000 (67%)]\tLoss: 0.213552\n",
            "Train Epoch: 5 [44928/60000 (75%)]\tLoss: 0.342924\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.216589\n",
            "Train Epoch: 5 [54912/60000 (92%)]\tLoss: 0.208039\n",
            "Train Epoch: 5 [59904/60000 (100%)]\tLoss: 0.433371\n",
            "\n",
            "Test set: Average loss: 0.4756, Accuracy: 8612/10000 (86%)\n",
            "Top-5 Accuracy: 0.9968\n",
            "Top-1 Accuracy: 0.8612\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.276611\n",
            "Train Epoch: 6 [4992/60000 (8%)]\tLoss: 0.220880\n",
            "Train Epoch: 6 [9984/60000 (17%)]\tLoss: 0.273737\n",
            "Train Epoch: 6 [14976/60000 (25%)]\tLoss: 0.285376\n",
            "Train Epoch: 6 [19968/60000 (33%)]\tLoss: 0.554173\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.226625\n",
            "Train Epoch: 6 [29952/60000 (50%)]\tLoss: 0.162399\n",
            "Train Epoch: 6 [34944/60000 (58%)]\tLoss: 0.330110\n",
            "Train Epoch: 6 [39936/60000 (67%)]\tLoss: 0.111789\n",
            "Train Epoch: 6 [44928/60000 (75%)]\tLoss: 0.327042\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.453193\n",
            "Train Epoch: 6 [54912/60000 (92%)]\tLoss: 0.366809\n",
            "Train Epoch: 6 [59904/60000 (100%)]\tLoss: 0.276590\n",
            "\n",
            "Test set: Average loss: 0.4184, Accuracy: 8645/10000 (86%)\n",
            "Top-5 Accuracy: 0.9969\n",
            "Top-1 Accuracy: 0.8645\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.378653\n",
            "Train Epoch: 7 [4992/60000 (8%)]\tLoss: 0.348333\n",
            "Train Epoch: 7 [9984/60000 (17%)]\tLoss: 0.211551\n",
            "Train Epoch: 7 [14976/60000 (25%)]\tLoss: 0.285125\n",
            "Train Epoch: 7 [19968/60000 (33%)]\tLoss: 0.243812\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.336374\n",
            "Train Epoch: 7 [29952/60000 (50%)]\tLoss: 0.448015\n",
            "Train Epoch: 7 [34944/60000 (58%)]\tLoss: 0.335424\n",
            "Train Epoch: 7 [39936/60000 (67%)]\tLoss: 0.339871\n",
            "Train Epoch: 7 [44928/60000 (75%)]\tLoss: 0.292399\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.438033\n",
            "Train Epoch: 7 [54912/60000 (92%)]\tLoss: 0.229694\n",
            "Train Epoch: 7 [59904/60000 (100%)]\tLoss: 0.226507\n",
            "\n",
            "Test set: Average loss: 0.3732, Accuracy: 8786/10000 (88%)\n",
            "Top-5 Accuracy: 0.9956\n",
            "Top-1 Accuracy: 0.8786\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.214364\n",
            "Train Epoch: 8 [4992/60000 (8%)]\tLoss: 0.340093\n",
            "Train Epoch: 8 [9984/60000 (17%)]\tLoss: 0.203045\n",
            "Train Epoch: 8 [14976/60000 (25%)]\tLoss: 0.392273\n",
            "Train Epoch: 8 [19968/60000 (33%)]\tLoss: 0.316733\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.450222\n",
            "Train Epoch: 8 [29952/60000 (50%)]\tLoss: 0.496345\n",
            "Train Epoch: 8 [34944/60000 (58%)]\tLoss: 0.349908\n",
            "Train Epoch: 8 [39936/60000 (67%)]\tLoss: 0.165282\n",
            "Train Epoch: 8 [44928/60000 (75%)]\tLoss: 0.087494\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.166966\n",
            "Train Epoch: 8 [54912/60000 (92%)]\tLoss: 0.263956\n",
            "Train Epoch: 8 [59904/60000 (100%)]\tLoss: 0.317817\n",
            "\n",
            "Test set: Average loss: 0.5026, Accuracy: 8775/10000 (88%)\n",
            "Top-5 Accuracy: 0.9978\n",
            "Top-1 Accuracy: 0.8775\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.311132\n",
            "Train Epoch: 9 [4992/60000 (8%)]\tLoss: 0.302848\n",
            "Train Epoch: 9 [9984/60000 (17%)]\tLoss: 0.393692\n",
            "Train Epoch: 9 [14976/60000 (25%)]\tLoss: 0.175286\n",
            "Train Epoch: 9 [19968/60000 (33%)]\tLoss: 0.296663\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.323887\n",
            "Train Epoch: 9 [29952/60000 (50%)]\tLoss: 0.144743\n",
            "Train Epoch: 9 [34944/60000 (58%)]\tLoss: 0.181830\n",
            "Train Epoch: 9 [39936/60000 (67%)]\tLoss: 0.209149\n",
            "Train Epoch: 9 [44928/60000 (75%)]\tLoss: 0.215941\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.322951\n",
            "Train Epoch: 9 [54912/60000 (92%)]\tLoss: 0.514392\n",
            "Train Epoch: 9 [59904/60000 (100%)]\tLoss: 0.090835\n",
            "\n",
            "Test set: Average loss: 0.4144, Accuracy: 8753/10000 (88%)\n",
            "Top-5 Accuracy: 0.9984\n",
            "Top-1 Accuracy: 0.8753\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.313668\n",
            "Train Epoch: 10 [4992/60000 (8%)]\tLoss: 0.295371\n",
            "Train Epoch: 10 [9984/60000 (17%)]\tLoss: 0.281461\n",
            "Train Epoch: 10 [14976/60000 (25%)]\tLoss: 0.339112\n",
            "Train Epoch: 10 [19968/60000 (33%)]\tLoss: 0.238705\n",
            "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.144077\n",
            "Train Epoch: 10 [29952/60000 (50%)]\tLoss: 0.258643\n",
            "Train Epoch: 10 [34944/60000 (58%)]\tLoss: 0.154300\n",
            "Train Epoch: 10 [39936/60000 (67%)]\tLoss: 0.207273\n",
            "Train Epoch: 10 [44928/60000 (75%)]\tLoss: 0.187086\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.389804\n",
            "Train Epoch: 10 [54912/60000 (92%)]\tLoss: 0.175693\n",
            "Train Epoch: 10 [59904/60000 (100%)]\tLoss: 0.136123\n",
            "\n",
            "Test set: Average loss: 0.3646, Accuracy: 8922/10000 (89%)\n",
            "Top-5 Accuracy: 0.9928\n",
            "Top-1 Accuracy: 0.8922\n",
            "\n",
            "10466.59769 sec\n",
            "ResNet(\n",
            "  664.97 M, 100.000% Params, 116.24 GMac, 100.000% MACs, \n",
            "  (conv1): Conv2d(3.14 k, 0.000% Params, 39.34 MMac, 0.034% MACs, 1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(128, 0.000% Params, 1.61 MMac, 0.001% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.001% MACs, inplace=True)\n",
            "  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.001% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    6.33 M, 0.953% Params, 19.91 GMac, 17.125% MACs, \n",
            "    (0): Bottleneck(\n",
            "      1.86 M, 0.280% Params, 5.85 GMac, 5.031% MACs, \n",
            "      (conv1): Conv2d(131.07 k, 0.020% Params, 411.04 MMac, 0.354% MACs, 64, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(4.1 k, 0.001% Params, 12.85 MMac, 0.011% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1.18 M, 0.177% Params, 3.7 GMac, 3.183% MACs, 2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(4.1 k, 0.001% Params, 12.85 MMac, 0.011% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(524.29 k, 0.079% Params, 1.64 GMac, 1.414% MACs, 2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 13.65 MMac, 0.012% MACs, inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        16.9 k, 0.003% Params, 52.99 MMac, 0.046% MACs, \n",
            "        (0): Conv2d(16.38 k, 0.002% Params, 51.38 MMac, 0.044% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      2.24 M, 0.336% Params, 7.03 GMac, 6.047% MACs, \n",
            "      (conv1): Conv2d(524.29 k, 0.079% Params, 1.64 GMac, 1.414% MACs, 256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(4.1 k, 0.001% Params, 12.85 MMac, 0.011% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1.18 M, 0.177% Params, 3.7 GMac, 3.183% MACs, 2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(4.1 k, 0.001% Params, 12.85 MMac, 0.011% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(524.29 k, 0.079% Params, 1.64 GMac, 1.414% MACs, 2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 13.65 MMac, 0.012% MACs, inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      2.24 M, 0.336% Params, 7.03 GMac, 6.047% MACs, \n",
            "      (conv1): Conv2d(524.29 k, 0.079% Params, 1.64 GMac, 1.414% MACs, 256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(4.1 k, 0.001% Params, 12.85 MMac, 0.011% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(1.18 M, 0.177% Params, 3.7 GMac, 3.183% MACs, 2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(4.1 k, 0.001% Params, 12.85 MMac, 0.011% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(524.29 k, 0.079% Params, 1.64 GMac, 1.414% MACs, 2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, 0.000% Params, 1.61 MMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 13.65 MMac, 0.012% MACs, inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    34.8 M, 5.234% Params, 29.81 GMac, 25.645% MACs, \n",
            "    (0): Bottleneck(\n",
            "      8.01 M, 1.205% Params, 8.78 GMac, 7.557% MACs, \n",
            "      (conv1): Conv2d(1.05 M, 0.158% Params, 3.29 GMac, 2.829% MACs, 256, 4096, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(8.19 k, 0.001% Params, 25.69 MMac, 0.022% MACs, 4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(4.72 M, 0.710% Params, 3.7 GMac, 3.183% MACs, 4096, 4096, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(8.19 k, 0.001% Params, 6.42 MMac, 0.006% MACs, 4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(2.1 M, 0.315% Params, 1.64 GMac, 1.414% MACs, 4096, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1.02 k, 0.000% Params, 802.82 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 16.46 MMac, 0.014% MACs, inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        132.1 k, 0.020% Params, 103.56 MMac, 0.089% MACs, \n",
            "        (0): Conv2d(131.07 k, 0.020% Params, 102.76 MMac, 0.088% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.000% Params, 802.82 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      8.93 M, 1.343% Params, 7.01 GMac, 6.029% MACs, \n",
            "      (conv1): Conv2d(2.1 M, 0.315% Params, 1.64 GMac, 1.414% MACs, 512, 4096, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(8.19 k, 0.001% Params, 6.42 MMac, 0.006% MACs, 4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(4.72 M, 0.710% Params, 3.7 GMac, 3.183% MACs, 4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(8.19 k, 0.001% Params, 6.42 MMac, 0.006% MACs, 4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(2.1 M, 0.315% Params, 1.64 GMac, 1.414% MACs, 4096, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1.02 k, 0.000% Params, 802.82 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 6.82 MMac, 0.006% MACs, inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      8.93 M, 1.343% Params, 7.01 GMac, 6.029% MACs, \n",
            "      (conv1): Conv2d(2.1 M, 0.315% Params, 1.64 GMac, 1.414% MACs, 512, 4096, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(8.19 k, 0.001% Params, 6.42 MMac, 0.006% MACs, 4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(4.72 M, 0.710% Params, 3.7 GMac, 3.183% MACs, 4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(8.19 k, 0.001% Params, 6.42 MMac, 0.006% MACs, 4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(2.1 M, 0.315% Params, 1.64 GMac, 1.414% MACs, 4096, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1.02 k, 0.000% Params, 802.82 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 6.82 MMac, 0.006% MACs, inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      8.93 M, 1.343% Params, 7.01 GMac, 6.029% MACs, \n",
            "      (conv1): Conv2d(2.1 M, 0.315% Params, 1.64 GMac, 1.414% MACs, 512, 4096, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(8.19 k, 0.001% Params, 6.42 MMac, 0.006% MACs, 4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(4.72 M, 0.710% Params, 3.7 GMac, 3.183% MACs, 4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(8.19 k, 0.001% Params, 6.42 MMac, 0.006% MACs, 4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(2.1 M, 0.315% Params, 1.64 GMac, 1.414% MACs, 4096, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1.02 k, 0.000% Params, 802.82 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 6.82 MMac, 0.006% MACs, inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    210.45 M, 31.648% Params, 43.75 GMac, 37.637% MACs, \n",
            "    (0): Bottleneck(\n",
            "      32.02 M, 4.815% Params, 8.76 GMac, 7.536% MACs, \n",
            "      (conv1): Conv2d(4.19 M, 0.631% Params, 3.29 GMac, 2.829% MACs, 512, 8192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16.38 k, 0.002% Params, 12.85 MMac, 0.011% MACs, 8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(18.87 M, 2.838% Params, 3.7 GMac, 3.183% MACs, 8192, 8192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(16.38 k, 0.002% Params, 3.21 MMac, 0.003% MACs, 8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(8.39 M, 1.262% Params, 1.64 GMac, 1.414% MACs, 8192, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2.05 k, 0.000% Params, 401.41 KMac, 0.000% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 8.23 MMac, 0.007% MACs, inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        526.34 k, 0.079% Params, 103.16 MMac, 0.089% MACs, \n",
            "        (0): Conv2d(524.29 k, 0.079% Params, 102.76 MMac, 0.088% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2.05 k, 0.000% Params, 401.41 KMac, 0.000% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      35.69 M, 5.367% Params, 7.0 GMac, 6.020% MACs, \n",
            "      (conv1): Conv2d(8.39 M, 1.262% Params, 1.64 GMac, 1.414% MACs, 1024, 8192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16.38 k, 0.002% Params, 3.21 MMac, 0.003% MACs, 8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(18.87 M, 2.838% Params, 3.7 GMac, 3.183% MACs, 8192, 8192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(16.38 k, 0.002% Params, 3.21 MMac, 0.003% MACs, 8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(8.39 M, 1.262% Params, 1.64 GMac, 1.414% MACs, 8192, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2.05 k, 0.000% Params, 401.41 KMac, 0.000% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 3.41 MMac, 0.003% MACs, inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      35.69 M, 5.367% Params, 7.0 GMac, 6.020% MACs, \n",
            "      (conv1): Conv2d(8.39 M, 1.262% Params, 1.64 GMac, 1.414% MACs, 1024, 8192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16.38 k, 0.002% Params, 3.21 MMac, 0.003% MACs, 8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(18.87 M, 2.838% Params, 3.7 GMac, 3.183% MACs, 8192, 8192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(16.38 k, 0.002% Params, 3.21 MMac, 0.003% MACs, 8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(8.39 M, 1.262% Params, 1.64 GMac, 1.414% MACs, 8192, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2.05 k, 0.000% Params, 401.41 KMac, 0.000% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 3.41 MMac, 0.003% MACs, inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      35.69 M, 5.367% Params, 7.0 GMac, 6.020% MACs, \n",
            "      (conv1): Conv2d(8.39 M, 1.262% Params, 1.64 GMac, 1.414% MACs, 1024, 8192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16.38 k, 0.002% Params, 3.21 MMac, 0.003% MACs, 8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(18.87 M, 2.838% Params, 3.7 GMac, 3.183% MACs, 8192, 8192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(16.38 k, 0.002% Params, 3.21 MMac, 0.003% MACs, 8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(8.39 M, 1.262% Params, 1.64 GMac, 1.414% MACs, 8192, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2.05 k, 0.000% Params, 401.41 KMac, 0.000% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 3.41 MMac, 0.003% MACs, inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      35.69 M, 5.367% Params, 7.0 GMac, 6.020% MACs, \n",
            "      (conv1): Conv2d(8.39 M, 1.262% Params, 1.64 GMac, 1.414% MACs, 1024, 8192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16.38 k, 0.002% Params, 3.21 MMac, 0.003% MACs, 8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(18.87 M, 2.838% Params, 3.7 GMac, 3.183% MACs, 8192, 8192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(16.38 k, 0.002% Params, 3.21 MMac, 0.003% MACs, 8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(8.39 M, 1.262% Params, 1.64 GMac, 1.414% MACs, 8192, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2.05 k, 0.000% Params, 401.41 KMac, 0.000% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 3.41 MMac, 0.003% MACs, inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      35.69 M, 5.367% Params, 7.0 GMac, 6.020% MACs, \n",
            "      (conv1): Conv2d(8.39 M, 1.262% Params, 1.64 GMac, 1.414% MACs, 1024, 8192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16.38 k, 0.002% Params, 3.21 MMac, 0.003% MACs, 8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(18.87 M, 2.838% Params, 3.7 GMac, 3.183% MACs, 8192, 8192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(16.38 k, 0.002% Params, 3.21 MMac, 0.003% MACs, 8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(8.39 M, 1.262% Params, 1.64 GMac, 1.414% MACs, 8192, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2.05 k, 0.000% Params, 401.41 KMac, 0.000% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 3.41 MMac, 0.003% MACs, inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    413.35 M, 62.161% Params, 22.73 GMac, 19.557% MACs, \n",
            "    (0): Bottleneck(\n",
            "      128.0 M, 19.249% Params, 8.75 GMac, 7.525% MACs, \n",
            "      (conv1): Conv2d(16.78 M, 2.523% Params, 3.29 GMac, 2.829% MACs, 1024, 16384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32.77 k, 0.005% Params, 6.42 MMac, 0.006% MACs, 16384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(75.5 M, 11.354% Params, 3.7 GMac, 3.183% MACs, 16384, 16384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(32.77 k, 0.005% Params, 1.61 MMac, 0.001% MACs, 16384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(33.55 M, 5.046% Params, 1.64 GMac, 1.414% MACs, 16384, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(4.1 k, 0.001% Params, 200.7 KMac, 0.000% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 4.11 MMac, 0.004% MACs, inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        2.1 M, 0.316% Params, 102.96 MMac, 0.089% MACs, \n",
            "        (0): Conv2d(2.1 M, 0.315% Params, 102.76 MMac, 0.088% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(4.1 k, 0.001% Params, 200.7 KMac, 0.000% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      142.68 M, 21.456% Params, 6.99 GMac, 6.016% MACs, \n",
            "      (conv1): Conv2d(33.55 M, 5.046% Params, 1.64 GMac, 1.414% MACs, 2048, 16384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32.77 k, 0.005% Params, 1.61 MMac, 0.001% MACs, 16384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(75.5 M, 11.354% Params, 3.7 GMac, 3.183% MACs, 16384, 16384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(32.77 k, 0.005% Params, 1.61 MMac, 0.001% MACs, 16384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(33.55 M, 5.046% Params, 1.64 GMac, 1.414% MACs, 16384, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(4.1 k, 0.001% Params, 200.7 KMac, 0.000% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 1.71 MMac, 0.001% MACs, inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      142.68 M, 21.456% Params, 6.99 GMac, 6.016% MACs, \n",
            "      (conv1): Conv2d(33.55 M, 5.046% Params, 1.64 GMac, 1.414% MACs, 2048, 16384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32.77 k, 0.005% Params, 1.61 MMac, 0.001% MACs, 16384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(75.5 M, 11.354% Params, 3.7 GMac, 3.183% MACs, 16384, 16384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "      (bn2): BatchNorm2d(32.77 k, 0.005% Params, 1.61 MMac, 0.001% MACs, 16384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(33.55 M, 5.046% Params, 1.64 GMac, 1.414% MACs, 16384, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(4.1 k, 0.001% Params, 200.7 KMac, 0.000% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 1.71 MMac, 0.001% MACs, inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (averpool): AdaptiveAvgPool2d(0, 0.000% Params, 100.35 KMac, 0.000% MACs, output_size=(1, 1))\n",
            "  (fc): Linear(20.49 k, 0.003% Params, 20.49 KMac, 0.000% MACs, in_features=2048, out_features=10, bias=True)\n",
            ")\n",
            "FLOPs: 116.24 GMac\n",
            "Parameters: 664.97 M\n",
            "[tensor(79.0200), tensor(82.6800), tensor(82.7400), tensor(82.8000), tensor(86.1200), tensor(86.4500), tensor(87.8600), tensor(87.7500), tensor(87.5300), tensor(89.2200)]\n",
            "[tensor(0.0833), tensor(0.0967), tensor(0.0933), tensor(0.0967), tensor(0.0917), tensor(0.0950), tensor(0.0983), tensor(0.1000), tensor(0.0933), tensor(0.0917)]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}