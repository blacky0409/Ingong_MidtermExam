{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOBj/jLHhmWo7vYIh7WAtMv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soccer1356-2000/Ingong_MidtermExam/blob/main/%EC%A4%91%EA%B0%84%EA%B3%BC%EC%A0%9C_201920778_%EC%9D%B4%EC%8A%B9%EC%9C%A4_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "FoWYQ5rfLSLH"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms # Fashion-MNIST dataset for PyTorch\n",
        "# Download and load the FashionMNIST training data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True,\n",
        "transform = transform)\n",
        "testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False,\n",
        "transform = transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "FbEBR-PBNQbC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset # 28 * 28 * 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZcKmEcXa6BP",
        "outputId": "8285b4f9-318d-403e-d786-ffa51fa0e27a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset FashionMNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: MNIST_data/\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5,), std=(0.5,))\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "6yjaet1MoX-Z"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, groups=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, groups=groups, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
      ],
      "metadata": {
        "id": "-lSZGD5wndIa"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64,  norm_layer=nn.BatchNorm2d):\n",
        "      \n",
        "        super().__init__()\n",
        "            \n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)  \n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "            \n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "BKX_QBynryp8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, norm_layer=nn.BatchNorm2d):\n",
        "        super().__init__()\n",
        "\n",
        "        # ResNext\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        \n",
        "\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups) # conv2에서 downsample\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        # 1x1 convolution layer\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        # 3x3 convolution layer\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        # 1x1 convolution layer\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        \n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "CimQEbX0sfJN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, layers, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, norm_layer=nn.BatchNorm2d):\n",
        "    super().__init__()\n",
        "    self.norm_layer = norm_layer\n",
        "    self.groups = groups\n",
        "    self.base_width = width_per_group\n",
        "\n",
        "    self.inplane = 64\n",
        "\n",
        "    #stage 1\n",
        "    self.conv1 = nn.Conv2d(1,self.inplane,kernel_size=7,stride = 2, padding = 3, bias = False)\n",
        "    self.bn1 = norm_layer(self.inplane)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "    #stage 2\n",
        "    self.layer1 = self.make_layer(block,64,layers[0])\n",
        "    #stage 3\n",
        "    self.layer2 = self.make_layer(block,128,layers[1],stride=2)\n",
        "    #stage 4\n",
        "    self.layer3 = self.make_layer(block,256,layers[2],stride=2)\n",
        "    #stage 5\n",
        "    self.layer4 = self.make_layer(block,512,layers[3],stride=2)\n",
        "    self.averpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(512 * block.expansion,10)\n",
        "  \n",
        "  def make_layer(self,block,palnes,blocks,stride = 1):\n",
        "    norm_layer = self.norm_layer\n",
        "\n",
        "    down_channel = None\n",
        "    \n",
        "    if stride != 1 or self.inplane != block.expansion * palnes:\n",
        "      down_channel = nn.Sequential(conv1x1(self.inplane, palnes * block.expansion,stride),norm_layer(palnes * block.expansion))\n",
        "\n",
        "    layers = []\n",
        "\n",
        "    layers.append(block(self.inplane, palnes, stride, down_channel, self.groups,self.base_width, norm_layer))\n",
        "    self.inplane = palnes * block.expansion # inplanes 업데이트\n",
        "    for _ in range(1, blocks):\n",
        "        layers.append(block(self.inplane, palnes, groups=self.groups,base_width=self.base_width, norm_layer=norm_layer))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "\n",
        "    x = self.averpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "   # output = nn.log_softmax(x, dim=1)\n",
        "    return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "yjZKnwgct0hB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "G_UN2T5709Rs"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args={}\n",
        "kwargs={}\n",
        "args['batch_size']=256\n",
        "args['epochs']=100  #The number of Epochs is the number of times you go through the full dataset. \n",
        "args['lr']=0.01 #Learning rate is how fast it will decend. \n",
        "args['momentum']=0.9 #SGD momentum (default: 0.5) Momentum is a moving average of our gradients (helps to keep direction).\n",
        "args['weight_decay']=1e-5\n",
        "\n",
        "args['seed']=1 #random seed\n",
        "args['log_interval']=5000 // args['batch_size']\n",
        "args['cuda']=True"
      ],
      "metadata": {
        "id": "QrIE9UAm0d0b"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "        if args['cuda']:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        #Variables in Pytorch are differenciable. \n",
        "        data, target = Variable(data), Variable(target)\n",
        "        #This will zero out the gradients for this batch. \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        train_pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        train_correct += train_pred.eq(target.data.view_as(train_pred)).long().cpu().sum()\n",
        "        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\n",
        "        loss = F.nll_loss(output, target)\n",
        "        #dloss/dx for every Variable \n",
        "        loss.backward()\n",
        "        #to do a one-step update on our parameter.\n",
        "        optimizer.step()\n",
        "        #Print out the loss periodically. \n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            train_correct = 0\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
        "                100. * batch_idx / len(trainloader), loss.data))\n",
        "    train_acc.insert(len(train_acc),(100. * train_correct / len(trainloader.dataset)))\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in testloader:\n",
        "        if args['cuda']:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
        "\n",
        "    test_loss /= len(testloader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(testloader.dataset),\n",
        "        100. * correct / len(testloader.dataset)))\n",
        "    test_acc.insert(len(test_acc),(100. * correct / len(testloader.dataset)))"
      ],
      "metadata": {
        "id": "URFhhVfUutlU"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from ptflops import get_model_complexity_info"
      ],
      "metadata": {
        "id": "6m_HDnysAp5c"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ptflops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9iyKkvXDkUc",
        "outputId": "48aadaf3-cfbc-4d9b-8412-cb72c832776f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ptflops in /usr/local/lib/python3.10/dist-packages (0.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->ptflops) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->ptflops) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ptflops) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ptflops) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(BasicBlock,[2,2,2,2])\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'],weight_decay=args['weight_decay'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "with torch.cuda.device(0):\n",
        "\n",
        "  input_tensor = torch.randn(1, 1, 224, 224).cuda()\n",
        "\n",
        "  flops, params = get_model_complexity_info(model, (1, 224, 224), as_strings=True, print_per_layer_stat=True)\n",
        "\n",
        "  print('FLOPs:', flops)\n",
        "\n",
        "  print('Parameters:', params)\n",
        "\n",
        "print(test_acc)\n",
        "print(train_acc)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot([i for i in range(1,len(test_acc)+1)], test_acc, color =\"blue\")\n",
        "a = plt.twinx()\n",
        "a.plot([i for i in range(1,len(train_acc)+1)], train_acc, color = \"red\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j6MJRqJ20YDM",
        "outputId": "818f6763-6d7f-4f8c-b30f-f6cb03913ac6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-d7fa55ca2732>:60: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.572900\n",
            "Train Epoch: 1 [4864/60000 (8%)]\tLoss: 0.576508\n",
            "Train Epoch: 1 [9728/60000 (16%)]\tLoss: 0.459969\n",
            "Train Epoch: 1 [14592/60000 (24%)]\tLoss: 0.420158\n",
            "Train Epoch: 1 [19456/60000 (32%)]\tLoss: 0.315443\n",
            "Train Epoch: 1 [24320/60000 (40%)]\tLoss: 0.403888\n",
            "Train Epoch: 1 [29184/60000 (49%)]\tLoss: 0.281956\n",
            "Train Epoch: 1 [34048/60000 (57%)]\tLoss: 0.421756\n",
            "Train Epoch: 1 [38912/60000 (65%)]\tLoss: 0.396075\n",
            "Train Epoch: 1 [43776/60000 (73%)]\tLoss: 0.336989\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.419881\n",
            "Train Epoch: 1 [53504/60000 (89%)]\tLoss: 0.383061\n",
            "Train Epoch: 1 [58368/60000 (97%)]\tLoss: 0.303372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-72-e61b35da3f21>:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.3462, Accuracy: 8738/10000 (87%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.318921\n",
            "Train Epoch: 2 [4864/60000 (8%)]\tLoss: 0.247944\n",
            "Train Epoch: 2 [9728/60000 (16%)]\tLoss: 0.321327\n",
            "Train Epoch: 2 [14592/60000 (24%)]\tLoss: 0.281965\n",
            "Train Epoch: 2 [19456/60000 (32%)]\tLoss: 0.270732\n",
            "Train Epoch: 2 [24320/60000 (40%)]\tLoss: 0.286308\n",
            "Train Epoch: 2 [29184/60000 (49%)]\tLoss: 0.315774\n",
            "Train Epoch: 2 [34048/60000 (57%)]\tLoss: 0.260522\n",
            "Train Epoch: 2 [38912/60000 (65%)]\tLoss: 0.298176\n",
            "Train Epoch: 2 [43776/60000 (73%)]\tLoss: 0.320290\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.270538\n",
            "Train Epoch: 2 [53504/60000 (89%)]\tLoss: 0.184823\n",
            "Train Epoch: 2 [58368/60000 (97%)]\tLoss: 0.326177\n",
            "\n",
            "Test set: Average loss: 0.2947, Accuracy: 8921/10000 (89%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.280534\n",
            "Train Epoch: 3 [4864/60000 (8%)]\tLoss: 0.195945\n",
            "Train Epoch: 3 [9728/60000 (16%)]\tLoss: 0.227602\n",
            "Train Epoch: 3 [14592/60000 (24%)]\tLoss: 0.219409\n",
            "Train Epoch: 3 [19456/60000 (32%)]\tLoss: 0.213044\n",
            "Train Epoch: 3 [24320/60000 (40%)]\tLoss: 0.298554\n",
            "Train Epoch: 3 [29184/60000 (49%)]\tLoss: 0.205269\n",
            "Train Epoch: 3 [34048/60000 (57%)]\tLoss: 0.258700\n",
            "Train Epoch: 3 [38912/60000 (65%)]\tLoss: 0.285740\n",
            "Train Epoch: 3 [43776/60000 (73%)]\tLoss: 0.316513\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.298960\n",
            "Train Epoch: 3 [53504/60000 (89%)]\tLoss: 0.326934\n",
            "Train Epoch: 3 [58368/60000 (97%)]\tLoss: 0.299605\n",
            "\n",
            "Test set: Average loss: 0.2883, Accuracy: 8952/10000 (90%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.202566\n",
            "Train Epoch: 4 [4864/60000 (8%)]\tLoss: 0.177186\n",
            "Train Epoch: 4 [9728/60000 (16%)]\tLoss: 0.196755\n",
            "Train Epoch: 4 [14592/60000 (24%)]\tLoss: 0.165897\n",
            "Train Epoch: 4 [19456/60000 (32%)]\tLoss: 0.181002\n",
            "Train Epoch: 4 [24320/60000 (40%)]\tLoss: 0.243249\n",
            "Train Epoch: 4 [29184/60000 (49%)]\tLoss: 0.190908\n",
            "Train Epoch: 4 [34048/60000 (57%)]\tLoss: 0.176442\n",
            "Train Epoch: 4 [38912/60000 (65%)]\tLoss: 0.218059\n",
            "Train Epoch: 4 [43776/60000 (73%)]\tLoss: 0.264789\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.180976\n",
            "Train Epoch: 4 [53504/60000 (89%)]\tLoss: 0.170734\n",
            "Train Epoch: 4 [58368/60000 (97%)]\tLoss: 0.215493\n",
            "\n",
            "Test set: Average loss: 0.2763, Accuracy: 8972/10000 (90%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.173833\n",
            "Train Epoch: 5 [4864/60000 (8%)]\tLoss: 0.106928\n",
            "Train Epoch: 5 [9728/60000 (16%)]\tLoss: 0.253571\n",
            "Train Epoch: 5 [14592/60000 (24%)]\tLoss: 0.190326\n",
            "Train Epoch: 5 [19456/60000 (32%)]\tLoss: 0.200810\n",
            "Train Epoch: 5 [24320/60000 (40%)]\tLoss: 0.182487\n",
            "Train Epoch: 5 [29184/60000 (49%)]\tLoss: 0.144985\n",
            "Train Epoch: 5 [34048/60000 (57%)]\tLoss: 0.142493\n",
            "Train Epoch: 5 [38912/60000 (65%)]\tLoss: 0.192048\n",
            "Train Epoch: 5 [43776/60000 (73%)]\tLoss: 0.216597\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.238801\n",
            "Train Epoch: 5 [53504/60000 (89%)]\tLoss: 0.272400\n",
            "Train Epoch: 5 [58368/60000 (97%)]\tLoss: 0.181422\n",
            "\n",
            "Test set: Average loss: 0.2694, Accuracy: 9029/10000 (90%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.156935\n",
            "Train Epoch: 6 [4864/60000 (8%)]\tLoss: 0.170303\n",
            "Train Epoch: 6 [9728/60000 (16%)]\tLoss: 0.135708\n",
            "Train Epoch: 6 [14592/60000 (24%)]\tLoss: 0.165212\n",
            "Train Epoch: 6 [19456/60000 (32%)]\tLoss: 0.202200\n",
            "Train Epoch: 6 [24320/60000 (40%)]\tLoss: 0.077909\n",
            "Train Epoch: 6 [29184/60000 (49%)]\tLoss: 0.091529\n",
            "Train Epoch: 6 [34048/60000 (57%)]\tLoss: 0.146041\n",
            "Train Epoch: 6 [38912/60000 (65%)]\tLoss: 0.121344\n",
            "Train Epoch: 6 [43776/60000 (73%)]\tLoss: 0.224434\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.118810\n",
            "Train Epoch: 6 [53504/60000 (89%)]\tLoss: 0.191945\n",
            "Train Epoch: 6 [58368/60000 (97%)]\tLoss: 0.210679\n",
            "\n",
            "Test set: Average loss: 0.3004, Accuracy: 8962/10000 (90%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.162359\n",
            "Train Epoch: 7 [4864/60000 (8%)]\tLoss: 0.135932\n",
            "Train Epoch: 7 [9728/60000 (16%)]\tLoss: 0.174643\n",
            "Train Epoch: 7 [14592/60000 (24%)]\tLoss: 0.134432\n",
            "Train Epoch: 7 [19456/60000 (32%)]\tLoss: 0.155952\n",
            "Train Epoch: 7 [24320/60000 (40%)]\tLoss: 0.130892\n",
            "Train Epoch: 7 [29184/60000 (49%)]\tLoss: 0.138883\n",
            "Train Epoch: 7 [34048/60000 (57%)]\tLoss: 0.286582\n",
            "Train Epoch: 7 [38912/60000 (65%)]\tLoss: 0.171424\n",
            "Train Epoch: 7 [43776/60000 (73%)]\tLoss: 0.186356\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.120536\n",
            "Train Epoch: 7 [53504/60000 (89%)]\tLoss: 0.149742\n",
            "Train Epoch: 7 [58368/60000 (97%)]\tLoss: 0.136884\n",
            "\n",
            "Test set: Average loss: 0.2979, Accuracy: 9005/10000 (90%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.112913\n",
            "Train Epoch: 8 [4864/60000 (8%)]\tLoss: 0.124173\n",
            "Train Epoch: 8 [9728/60000 (16%)]\tLoss: 0.096511\n",
            "Train Epoch: 8 [14592/60000 (24%)]\tLoss: 0.093897\n",
            "Train Epoch: 8 [19456/60000 (32%)]\tLoss: 0.241904\n",
            "Train Epoch: 8 [24320/60000 (40%)]\tLoss: 0.081938\n",
            "Train Epoch: 8 [29184/60000 (49%)]\tLoss: 0.101872\n",
            "Train Epoch: 8 [34048/60000 (57%)]\tLoss: 0.112388\n",
            "Train Epoch: 8 [38912/60000 (65%)]\tLoss: 0.158356\n",
            "Train Epoch: 8 [43776/60000 (73%)]\tLoss: 0.115193\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.187616\n",
            "Train Epoch: 8 [53504/60000 (89%)]\tLoss: 0.087850\n",
            "Train Epoch: 8 [58368/60000 (97%)]\tLoss: 0.132437\n",
            "\n",
            "Test set: Average loss: 0.2925, Accuracy: 9027/10000 (90%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.124692\n",
            "Train Epoch: 9 [4864/60000 (8%)]\tLoss: 0.074348\n",
            "Train Epoch: 9 [9728/60000 (16%)]\tLoss: 0.099767\n",
            "Train Epoch: 9 [14592/60000 (24%)]\tLoss: 0.118511\n",
            "Train Epoch: 9 [19456/60000 (32%)]\tLoss: 0.068234\n",
            "Train Epoch: 9 [24320/60000 (40%)]\tLoss: 0.097001\n",
            "Train Epoch: 9 [29184/60000 (49%)]\tLoss: 0.105777\n",
            "Train Epoch: 9 [34048/60000 (57%)]\tLoss: 0.144894\n",
            "Train Epoch: 9 [38912/60000 (65%)]\tLoss: 0.112823\n",
            "Train Epoch: 9 [43776/60000 (73%)]\tLoss: 0.095165\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.128385\n",
            "Train Epoch: 9 [53504/60000 (89%)]\tLoss: 0.138849\n",
            "Train Epoch: 9 [58368/60000 (97%)]\tLoss: 0.137971\n",
            "\n",
            "Test set: Average loss: 0.2844, Accuracy: 9072/10000 (91%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.096294\n",
            "Train Epoch: 10 [4864/60000 (8%)]\tLoss: 0.094650\n",
            "Train Epoch: 10 [9728/60000 (16%)]\tLoss: 0.103054\n",
            "Train Epoch: 10 [14592/60000 (24%)]\tLoss: 0.074275\n",
            "Train Epoch: 10 [19456/60000 (32%)]\tLoss: 0.126944\n",
            "Train Epoch: 10 [24320/60000 (40%)]\tLoss: 0.057480\n",
            "Train Epoch: 10 [29184/60000 (49%)]\tLoss: 0.106314\n",
            "Train Epoch: 10 [34048/60000 (57%)]\tLoss: 0.140874\n",
            "Train Epoch: 10 [38912/60000 (65%)]\tLoss: 0.106767\n",
            "Train Epoch: 10 [43776/60000 (73%)]\tLoss: 0.129843\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.116646\n",
            "Train Epoch: 10 [53504/60000 (89%)]\tLoss: 0.142207\n",
            "Train Epoch: 10 [58368/60000 (97%)]\tLoss: 0.137805\n",
            "\n",
            "Test set: Average loss: 0.3198, Accuracy: 9056/10000 (91%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.091243\n",
            "Train Epoch: 11 [4864/60000 (8%)]\tLoss: 0.113567\n",
            "Train Epoch: 11 [9728/60000 (16%)]\tLoss: 0.066079\n",
            "Train Epoch: 11 [14592/60000 (24%)]\tLoss: 0.071549\n",
            "Train Epoch: 11 [19456/60000 (32%)]\tLoss: 0.059870\n",
            "Train Epoch: 11 [24320/60000 (40%)]\tLoss: 0.083607\n",
            "Train Epoch: 11 [29184/60000 (49%)]\tLoss: 0.118308\n",
            "Train Epoch: 11 [34048/60000 (57%)]\tLoss: 0.085428\n",
            "Train Epoch: 11 [38912/60000 (65%)]\tLoss: 0.091439\n",
            "Train Epoch: 11 [43776/60000 (73%)]\tLoss: 0.168977\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.107015\n",
            "Train Epoch: 11 [53504/60000 (89%)]\tLoss: 0.083545\n",
            "Train Epoch: 11 [58368/60000 (97%)]\tLoss: 0.112941\n",
            "\n",
            "Test set: Average loss: 0.3047, Accuracy: 9071/10000 (91%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.067783\n",
            "Train Epoch: 12 [4864/60000 (8%)]\tLoss: 0.113448\n",
            "Train Epoch: 12 [9728/60000 (16%)]\tLoss: 0.149855\n",
            "Train Epoch: 12 [14592/60000 (24%)]\tLoss: 0.059613\n",
            "Train Epoch: 12 [19456/60000 (32%)]\tLoss: 0.116122\n",
            "Train Epoch: 12 [24320/60000 (40%)]\tLoss: 0.134967\n",
            "Train Epoch: 12 [29184/60000 (49%)]\tLoss: 0.062790\n",
            "Train Epoch: 12 [34048/60000 (57%)]\tLoss: 0.086406\n",
            "Train Epoch: 12 [38912/60000 (65%)]\tLoss: 0.087815\n",
            "Train Epoch: 12 [43776/60000 (73%)]\tLoss: 0.118639\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.085583\n",
            "Train Epoch: 12 [53504/60000 (89%)]\tLoss: 0.089293\n",
            "Train Epoch: 12 [58368/60000 (97%)]\tLoss: 0.134273\n",
            "\n",
            "Test set: Average loss: 0.3485, Accuracy: 9001/10000 (90%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.077066\n",
            "Train Epoch: 13 [4864/60000 (8%)]\tLoss: 0.083528\n",
            "Train Epoch: 13 [9728/60000 (16%)]\tLoss: 0.073742\n",
            "Train Epoch: 13 [14592/60000 (24%)]\tLoss: 0.065403\n",
            "Train Epoch: 13 [19456/60000 (32%)]\tLoss: 0.077338\n",
            "Train Epoch: 13 [24320/60000 (40%)]\tLoss: 0.081670\n",
            "Train Epoch: 13 [29184/60000 (49%)]\tLoss: 0.113012\n",
            "Train Epoch: 13 [34048/60000 (57%)]\tLoss: 0.066466\n",
            "Train Epoch: 13 [38912/60000 (65%)]\tLoss: 0.092947\n",
            "Train Epoch: 13 [43776/60000 (73%)]\tLoss: 0.149793\n",
            "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.059601\n",
            "Train Epoch: 13 [53504/60000 (89%)]\tLoss: 0.051292\n",
            "Train Epoch: 13 [58368/60000 (97%)]\tLoss: 0.093276\n",
            "\n",
            "Test set: Average loss: 0.3330, Accuracy: 9077/10000 (91%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.046100\n",
            "Train Epoch: 14 [4864/60000 (8%)]\tLoss: 0.052179\n",
            "Train Epoch: 14 [9728/60000 (16%)]\tLoss: 0.099209\n",
            "Train Epoch: 14 [14592/60000 (24%)]\tLoss: 0.102525\n",
            "Train Epoch: 14 [19456/60000 (32%)]\tLoss: 0.046725\n",
            "Train Epoch: 14 [24320/60000 (40%)]\tLoss: 0.070469\n",
            "Train Epoch: 14 [29184/60000 (49%)]\tLoss: 0.103206\n",
            "Train Epoch: 14 [34048/60000 (57%)]\tLoss: 0.062908\n",
            "Train Epoch: 14 [38912/60000 (65%)]\tLoss: 0.078152\n",
            "Train Epoch: 14 [43776/60000 (73%)]\tLoss: 0.097211\n",
            "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.090241\n",
            "Train Epoch: 14 [53504/60000 (89%)]\tLoss: 0.067938\n",
            "Train Epoch: 14 [58368/60000 (97%)]\tLoss: 0.062015\n",
            "\n",
            "Test set: Average loss: 0.3681, Accuracy: 9032/10000 (90%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.079925\n",
            "Train Epoch: 15 [4864/60000 (8%)]\tLoss: 0.041512\n",
            "Train Epoch: 15 [9728/60000 (16%)]\tLoss: 0.037705\n",
            "Train Epoch: 15 [14592/60000 (24%)]\tLoss: 0.060065\n",
            "Train Epoch: 15 [19456/60000 (32%)]\tLoss: 0.049074\n",
            "Train Epoch: 15 [24320/60000 (40%)]\tLoss: 0.067592\n",
            "Train Epoch: 15 [29184/60000 (49%)]\tLoss: 0.059763\n",
            "Train Epoch: 15 [34048/60000 (57%)]\tLoss: 0.072061\n",
            "Train Epoch: 15 [38912/60000 (65%)]\tLoss: 0.078880\n",
            "Train Epoch: 15 [43776/60000 (73%)]\tLoss: 0.050194\n",
            "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 0.108555\n",
            "Train Epoch: 15 [53504/60000 (89%)]\tLoss: 0.074757\n",
            "Train Epoch: 15 [58368/60000 (97%)]\tLoss: 0.045210\n",
            "\n",
            "Test set: Average loss: 0.3731, Accuracy: 8981/10000 (90%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.079147\n",
            "Train Epoch: 16 [4864/60000 (8%)]\tLoss: 0.035919\n",
            "Train Epoch: 16 [9728/60000 (16%)]\tLoss: 0.038069\n",
            "Train Epoch: 16 [14592/60000 (24%)]\tLoss: 0.026177\n",
            "Train Epoch: 16 [19456/60000 (32%)]\tLoss: 0.063655\n",
            "Train Epoch: 16 [24320/60000 (40%)]\tLoss: 0.046586\n",
            "Train Epoch: 16 [29184/60000 (49%)]\tLoss: 0.054567\n",
            "Train Epoch: 16 [34048/60000 (57%)]\tLoss: 0.051062\n",
            "Train Epoch: 16 [38912/60000 (65%)]\tLoss: 0.059585\n",
            "Train Epoch: 16 [43776/60000 (73%)]\tLoss: 0.079488\n",
            "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 0.032947\n",
            "Train Epoch: 16 [53504/60000 (89%)]\tLoss: 0.078428\n",
            "Train Epoch: 16 [58368/60000 (97%)]\tLoss: 0.082649\n",
            "\n",
            "Test set: Average loss: 0.3737, Accuracy: 9039/10000 (90%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.078651\n",
            "Train Epoch: 17 [4864/60000 (8%)]\tLoss: 0.088469\n",
            "Train Epoch: 17 [9728/60000 (16%)]\tLoss: 0.043297\n",
            "Train Epoch: 17 [14592/60000 (24%)]\tLoss: 0.059196\n",
            "Train Epoch: 17 [19456/60000 (32%)]\tLoss: 0.027906\n",
            "Train Epoch: 17 [24320/60000 (40%)]\tLoss: 0.054006\n",
            "Train Epoch: 17 [29184/60000 (49%)]\tLoss: 0.024612\n",
            "Train Epoch: 17 [34048/60000 (57%)]\tLoss: 0.054811\n",
            "Train Epoch: 17 [38912/60000 (65%)]\tLoss: 0.056556\n",
            "Train Epoch: 17 [43776/60000 (73%)]\tLoss: 0.028788\n",
            "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 0.046094\n",
            "Train Epoch: 17 [53504/60000 (89%)]\tLoss: 0.058060\n",
            "Train Epoch: 17 [58368/60000 (97%)]\tLoss: 0.092503\n",
            "\n",
            "Test set: Average loss: 0.3704, Accuracy: 9055/10000 (91%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.038652\n",
            "Train Epoch: 18 [4864/60000 (8%)]\tLoss: 0.050793\n",
            "Train Epoch: 18 [9728/60000 (16%)]\tLoss: 0.064905\n",
            "Train Epoch: 18 [14592/60000 (24%)]\tLoss: 0.065371\n",
            "Train Epoch: 18 [19456/60000 (32%)]\tLoss: 0.048633\n",
            "Train Epoch: 18 [24320/60000 (40%)]\tLoss: 0.064209\n",
            "Train Epoch: 18 [29184/60000 (49%)]\tLoss: 0.027854\n",
            "Train Epoch: 18 [34048/60000 (57%)]\tLoss: 0.028749\n",
            "Train Epoch: 18 [38912/60000 (65%)]\tLoss: 0.061169\n",
            "Train Epoch: 18 [43776/60000 (73%)]\tLoss: 0.062044\n",
            "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 0.056486\n",
            "Train Epoch: 18 [53504/60000 (89%)]\tLoss: 0.055780\n",
            "Train Epoch: 18 [58368/60000 (97%)]\tLoss: 0.049938\n",
            "\n",
            "Test set: Average loss: 0.4151, Accuracy: 9057/10000 (91%)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.093764\n",
            "Train Epoch: 19 [4864/60000 (8%)]\tLoss: 0.039669\n",
            "Train Epoch: 19 [9728/60000 (16%)]\tLoss: 0.038737\n",
            "Train Epoch: 19 [14592/60000 (24%)]\tLoss: 0.059697\n",
            "Train Epoch: 19 [19456/60000 (32%)]\tLoss: 0.061017\n",
            "Train Epoch: 19 [24320/60000 (40%)]\tLoss: 0.089311\n",
            "Train Epoch: 19 [29184/60000 (49%)]\tLoss: 0.056857\n",
            "Train Epoch: 19 [34048/60000 (57%)]\tLoss: 0.043295\n",
            "Train Epoch: 19 [38912/60000 (65%)]\tLoss: 0.101813\n",
            "Train Epoch: 19 [43776/60000 (73%)]\tLoss: 0.056787\n",
            "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 0.076035\n",
            "Train Epoch: 19 [53504/60000 (89%)]\tLoss: 0.033823\n",
            "Train Epoch: 19 [58368/60000 (97%)]\tLoss: 0.047692\n",
            "\n",
            "Test set: Average loss: 0.3931, Accuracy: 9065/10000 (91%)\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.017441\n",
            "Train Epoch: 20 [4864/60000 (8%)]\tLoss: 0.029739\n",
            "Train Epoch: 20 [9728/60000 (16%)]\tLoss: 0.030307\n",
            "Train Epoch: 20 [14592/60000 (24%)]\tLoss: 0.018892\n",
            "Train Epoch: 20 [19456/60000 (32%)]\tLoss: 0.029396\n",
            "Train Epoch: 20 [24320/60000 (40%)]\tLoss: 0.017434\n",
            "Train Epoch: 20 [29184/60000 (49%)]\tLoss: 0.049844\n",
            "Train Epoch: 20 [34048/60000 (57%)]\tLoss: 0.047243\n",
            "Train Epoch: 20 [38912/60000 (65%)]\tLoss: 0.010423\n",
            "Train Epoch: 20 [43776/60000 (73%)]\tLoss: 0.055465\n",
            "Train Epoch: 20 [48640/60000 (81%)]\tLoss: 0.031742\n",
            "Train Epoch: 20 [53504/60000 (89%)]\tLoss: 0.040163\n",
            "Train Epoch: 20 [58368/60000 (97%)]\tLoss: 0.029304\n",
            "\n",
            "Test set: Average loss: 0.4374, Accuracy: 9087/10000 (91%)\n",
            "\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.019563\n",
            "Train Epoch: 21 [4864/60000 (8%)]\tLoss: 0.029352\n",
            "Train Epoch: 21 [9728/60000 (16%)]\tLoss: 0.025191\n",
            "Train Epoch: 21 [14592/60000 (24%)]\tLoss: 0.012948\n",
            "Train Epoch: 21 [19456/60000 (32%)]\tLoss: 0.018495\n",
            "Train Epoch: 21 [24320/60000 (40%)]\tLoss: 0.017617\n",
            "Train Epoch: 21 [29184/60000 (49%)]\tLoss: 0.023506\n",
            "Train Epoch: 21 [34048/60000 (57%)]\tLoss: 0.015470\n",
            "Train Epoch: 21 [38912/60000 (65%)]\tLoss: 0.059734\n",
            "Train Epoch: 21 [43776/60000 (73%)]\tLoss: 0.027565\n",
            "Train Epoch: 21 [48640/60000 (81%)]\tLoss: 0.017846\n",
            "Train Epoch: 21 [53504/60000 (89%)]\tLoss: 0.017538\n",
            "Train Epoch: 21 [58368/60000 (97%)]\tLoss: 0.038138\n",
            "\n",
            "Test set: Average loss: 0.4290, Accuracy: 9073/10000 (91%)\n",
            "\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.023813\n",
            "Train Epoch: 22 [4864/60000 (8%)]\tLoss: 0.032832\n",
            "Train Epoch: 22 [9728/60000 (16%)]\tLoss: 0.009329\n",
            "Train Epoch: 22 [14592/60000 (24%)]\tLoss: 0.030959\n",
            "Train Epoch: 22 [19456/60000 (32%)]\tLoss: 0.087885\n",
            "Train Epoch: 22 [24320/60000 (40%)]\tLoss: 0.048002\n",
            "Train Epoch: 22 [29184/60000 (49%)]\tLoss: 0.048951\n",
            "Train Epoch: 22 [34048/60000 (57%)]\tLoss: 0.038381\n",
            "Train Epoch: 22 [38912/60000 (65%)]\tLoss: 0.023743\n",
            "Train Epoch: 22 [43776/60000 (73%)]\tLoss: 0.019752\n",
            "Train Epoch: 22 [48640/60000 (81%)]\tLoss: 0.027520\n",
            "Train Epoch: 22 [53504/60000 (89%)]\tLoss: 0.016411\n",
            "Train Epoch: 22 [58368/60000 (97%)]\tLoss: 0.046249\n",
            "\n",
            "Test set: Average loss: 0.4582, Accuracy: 9031/10000 (90%)\n",
            "\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.032702\n",
            "Train Epoch: 23 [4864/60000 (8%)]\tLoss: 0.017918\n",
            "Train Epoch: 23 [9728/60000 (16%)]\tLoss: 0.016835\n",
            "Train Epoch: 23 [14592/60000 (24%)]\tLoss: 0.017069\n",
            "Train Epoch: 23 [19456/60000 (32%)]\tLoss: 0.065076\n",
            "Train Epoch: 23 [24320/60000 (40%)]\tLoss: 0.005896\n",
            "Train Epoch: 23 [29184/60000 (49%)]\tLoss: 0.037424\n",
            "Train Epoch: 23 [34048/60000 (57%)]\tLoss: 0.037520\n",
            "Train Epoch: 23 [38912/60000 (65%)]\tLoss: 0.026322\n",
            "Train Epoch: 23 [43776/60000 (73%)]\tLoss: 0.060386\n",
            "Train Epoch: 23 [48640/60000 (81%)]\tLoss: 0.057932\n",
            "Train Epoch: 23 [53504/60000 (89%)]\tLoss: 0.028833\n",
            "Train Epoch: 23 [58368/60000 (97%)]\tLoss: 0.054755\n",
            "\n",
            "Test set: Average loss: 0.4517, Accuracy: 9050/10000 (90%)\n",
            "\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.037716\n",
            "Train Epoch: 24 [4864/60000 (8%)]\tLoss: 0.017735\n",
            "Train Epoch: 24 [9728/60000 (16%)]\tLoss: 0.061353\n",
            "Train Epoch: 24 [14592/60000 (24%)]\tLoss: 0.019393\n",
            "Train Epoch: 24 [19456/60000 (32%)]\tLoss: 0.017669\n",
            "Train Epoch: 24 [24320/60000 (40%)]\tLoss: 0.028149\n",
            "Train Epoch: 24 [29184/60000 (49%)]\tLoss: 0.027744\n",
            "Train Epoch: 24 [34048/60000 (57%)]\tLoss: 0.014499\n",
            "Train Epoch: 24 [38912/60000 (65%)]\tLoss: 0.025314\n",
            "Train Epoch: 24 [43776/60000 (73%)]\tLoss: 0.008586\n",
            "Train Epoch: 24 [48640/60000 (81%)]\tLoss: 0.012060\n",
            "Train Epoch: 24 [53504/60000 (89%)]\tLoss: 0.050793\n",
            "Train Epoch: 24 [58368/60000 (97%)]\tLoss: 0.018498\n",
            "\n",
            "Test set: Average loss: 0.4428, Accuracy: 9093/10000 (91%)\n",
            "\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.019617\n",
            "Train Epoch: 25 [4864/60000 (8%)]\tLoss: 0.014580\n",
            "Train Epoch: 25 [9728/60000 (16%)]\tLoss: 0.010484\n",
            "Train Epoch: 25 [14592/60000 (24%)]\tLoss: 0.016027\n",
            "Train Epoch: 25 [19456/60000 (32%)]\tLoss: 0.047558\n",
            "Train Epoch: 25 [24320/60000 (40%)]\tLoss: 0.026442\n",
            "Train Epoch: 25 [29184/60000 (49%)]\tLoss: 0.015246\n",
            "Train Epoch: 25 [34048/60000 (57%)]\tLoss: 0.027904\n",
            "Train Epoch: 25 [38912/60000 (65%)]\tLoss: 0.025237\n",
            "Train Epoch: 25 [43776/60000 (73%)]\tLoss: 0.006739\n",
            "Train Epoch: 25 [48640/60000 (81%)]\tLoss: 0.026272\n",
            "Train Epoch: 25 [53504/60000 (89%)]\tLoss: 0.067053\n",
            "Train Epoch: 25 [58368/60000 (97%)]\tLoss: 0.029393\n",
            "\n",
            "Test set: Average loss: 0.4398, Accuracy: 9098/10000 (91%)\n",
            "\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.007483\n",
            "Train Epoch: 26 [4864/60000 (8%)]\tLoss: 0.013705\n",
            "Train Epoch: 26 [9728/60000 (16%)]\tLoss: 0.050782\n",
            "Train Epoch: 26 [14592/60000 (24%)]\tLoss: 0.017944\n",
            "Train Epoch: 26 [19456/60000 (32%)]\tLoss: 0.023856\n",
            "Train Epoch: 26 [24320/60000 (40%)]\tLoss: 0.012649\n",
            "Train Epoch: 26 [29184/60000 (49%)]\tLoss: 0.016853\n",
            "Train Epoch: 26 [34048/60000 (57%)]\tLoss: 0.036676\n",
            "Train Epoch: 26 [38912/60000 (65%)]\tLoss: 0.029547\n",
            "Train Epoch: 26 [43776/60000 (73%)]\tLoss: 0.017707\n",
            "Train Epoch: 26 [48640/60000 (81%)]\tLoss: 0.022063\n",
            "Train Epoch: 26 [53504/60000 (89%)]\tLoss: 0.020249\n",
            "Train Epoch: 26 [58368/60000 (97%)]\tLoss: 0.025874\n",
            "\n",
            "Test set: Average loss: 0.4671, Accuracy: 9099/10000 (91%)\n",
            "\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.034731\n",
            "Train Epoch: 27 [4864/60000 (8%)]\tLoss: 0.010831\n",
            "Train Epoch: 27 [9728/60000 (16%)]\tLoss: 0.015313\n",
            "Train Epoch: 27 [14592/60000 (24%)]\tLoss: 0.004890\n",
            "Train Epoch: 27 [19456/60000 (32%)]\tLoss: 0.010663\n",
            "Train Epoch: 27 [24320/60000 (40%)]\tLoss: 0.016771\n",
            "Train Epoch: 27 [29184/60000 (49%)]\tLoss: 0.048277\n",
            "Train Epoch: 27 [34048/60000 (57%)]\tLoss: 0.023853\n",
            "Train Epoch: 27 [38912/60000 (65%)]\tLoss: 0.034820\n",
            "Train Epoch: 27 [43776/60000 (73%)]\tLoss: 0.038210\n",
            "Train Epoch: 27 [48640/60000 (81%)]\tLoss: 0.025018\n",
            "Train Epoch: 27 [53504/60000 (89%)]\tLoss: 0.024501\n",
            "Train Epoch: 27 [58368/60000 (97%)]\tLoss: 0.029557\n",
            "\n",
            "Test set: Average loss: 0.4475, Accuracy: 9107/10000 (91%)\n",
            "\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.004997\n",
            "Train Epoch: 28 [4864/60000 (8%)]\tLoss: 0.022382\n",
            "Train Epoch: 28 [9728/60000 (16%)]\tLoss: 0.010240\n",
            "Train Epoch: 28 [14592/60000 (24%)]\tLoss: 0.021163\n",
            "Train Epoch: 28 [19456/60000 (32%)]\tLoss: 0.019458\n",
            "Train Epoch: 28 [24320/60000 (40%)]\tLoss: 0.019452\n",
            "Train Epoch: 28 [29184/60000 (49%)]\tLoss: 0.008377\n",
            "Train Epoch: 28 [34048/60000 (57%)]\tLoss: 0.020013\n",
            "Train Epoch: 28 [38912/60000 (65%)]\tLoss: 0.012743\n",
            "Train Epoch: 28 [43776/60000 (73%)]\tLoss: 0.040543\n",
            "Train Epoch: 28 [48640/60000 (81%)]\tLoss: 0.016906\n",
            "Train Epoch: 28 [53504/60000 (89%)]\tLoss: 0.013321\n",
            "Train Epoch: 28 [58368/60000 (97%)]\tLoss: 0.008974\n",
            "\n",
            "Test set: Average loss: 0.4996, Accuracy: 9080/10000 (91%)\n",
            "\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.012495\n",
            "Train Epoch: 29 [4864/60000 (8%)]\tLoss: 0.008913\n",
            "Train Epoch: 29 [9728/60000 (16%)]\tLoss: 0.018425\n",
            "Train Epoch: 29 [14592/60000 (24%)]\tLoss: 0.015987\n",
            "Train Epoch: 29 [19456/60000 (32%)]\tLoss: 0.010659\n",
            "Train Epoch: 29 [24320/60000 (40%)]\tLoss: 0.024221\n",
            "Train Epoch: 29 [29184/60000 (49%)]\tLoss: 0.051491\n",
            "Train Epoch: 29 [34048/60000 (57%)]\tLoss: 0.067244\n",
            "Train Epoch: 29 [38912/60000 (65%)]\tLoss: 0.029855\n",
            "Train Epoch: 29 [43776/60000 (73%)]\tLoss: 0.021558\n",
            "Train Epoch: 29 [48640/60000 (81%)]\tLoss: 0.037953\n",
            "Train Epoch: 29 [53504/60000 (89%)]\tLoss: 0.008577\n",
            "Train Epoch: 29 [58368/60000 (97%)]\tLoss: 0.010341\n",
            "\n",
            "Test set: Average loss: 0.4811, Accuracy: 9143/10000 (91%)\n",
            "\n",
            "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.007041\n",
            "Train Epoch: 30 [4864/60000 (8%)]\tLoss: 0.006608\n",
            "Train Epoch: 30 [9728/60000 (16%)]\tLoss: 0.003650\n",
            "Train Epoch: 30 [14592/60000 (24%)]\tLoss: 0.024727\n",
            "Train Epoch: 30 [19456/60000 (32%)]\tLoss: 0.005132\n",
            "Train Epoch: 30 [24320/60000 (40%)]\tLoss: 0.010580\n",
            "Train Epoch: 30 [29184/60000 (49%)]\tLoss: 0.039568\n",
            "Train Epoch: 30 [34048/60000 (57%)]\tLoss: 0.013282\n",
            "Train Epoch: 30 [38912/60000 (65%)]\tLoss: 0.019692\n",
            "Train Epoch: 30 [43776/60000 (73%)]\tLoss: 0.023065\n",
            "Train Epoch: 30 [48640/60000 (81%)]\tLoss: 0.019154\n",
            "Train Epoch: 30 [53504/60000 (89%)]\tLoss: 0.011334\n",
            "Train Epoch: 30 [58368/60000 (97%)]\tLoss: 0.018303\n",
            "\n",
            "Test set: Average loss: 0.4751, Accuracy: 9137/10000 (91%)\n",
            "\n",
            "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.023602\n",
            "Train Epoch: 31 [4864/60000 (8%)]\tLoss: 0.009983\n",
            "Train Epoch: 31 [9728/60000 (16%)]\tLoss: 0.015682\n",
            "Train Epoch: 31 [14592/60000 (24%)]\tLoss: 0.004899\n",
            "Train Epoch: 31 [19456/60000 (32%)]\tLoss: 0.008814\n",
            "Train Epoch: 31 [24320/60000 (40%)]\tLoss: 0.000495\n",
            "Train Epoch: 31 [29184/60000 (49%)]\tLoss: 0.012146\n",
            "Train Epoch: 31 [34048/60000 (57%)]\tLoss: 0.011403\n",
            "Train Epoch: 31 [38912/60000 (65%)]\tLoss: 0.045675\n",
            "Train Epoch: 31 [43776/60000 (73%)]\tLoss: 0.025028\n",
            "Train Epoch: 31 [48640/60000 (81%)]\tLoss: 0.027609\n",
            "Train Epoch: 31 [53504/60000 (89%)]\tLoss: 0.009063\n",
            "Train Epoch: 31 [58368/60000 (97%)]\tLoss: 0.025471\n",
            "\n",
            "Test set: Average loss: 0.5351, Accuracy: 9106/10000 (91%)\n",
            "\n",
            "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.007793\n",
            "Train Epoch: 32 [4864/60000 (8%)]\tLoss: 0.010183\n",
            "Train Epoch: 32 [9728/60000 (16%)]\tLoss: 0.028666\n",
            "Train Epoch: 32 [14592/60000 (24%)]\tLoss: 0.005213\n",
            "Train Epoch: 32 [19456/60000 (32%)]\tLoss: 0.021737\n",
            "Train Epoch: 32 [24320/60000 (40%)]\tLoss: 0.003324\n",
            "Train Epoch: 32 [29184/60000 (49%)]\tLoss: 0.019972\n",
            "Train Epoch: 32 [34048/60000 (57%)]\tLoss: 0.016274\n",
            "Train Epoch: 32 [38912/60000 (65%)]\tLoss: 0.004253\n",
            "Train Epoch: 32 [43776/60000 (73%)]\tLoss: 0.003851\n",
            "Train Epoch: 32 [48640/60000 (81%)]\tLoss: 0.014662\n",
            "Train Epoch: 32 [53504/60000 (89%)]\tLoss: 0.015301\n",
            "Train Epoch: 32 [58368/60000 (97%)]\tLoss: 0.009810\n",
            "\n",
            "Test set: Average loss: 0.5089, Accuracy: 9124/10000 (91%)\n",
            "\n",
            "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.015678\n",
            "Train Epoch: 33 [4864/60000 (8%)]\tLoss: 0.010319\n",
            "Train Epoch: 33 [9728/60000 (16%)]\tLoss: 0.001786\n",
            "Train Epoch: 33 [14592/60000 (24%)]\tLoss: 0.004004\n",
            "Train Epoch: 33 [19456/60000 (32%)]\tLoss: 0.006235\n",
            "Train Epoch: 33 [24320/60000 (40%)]\tLoss: 0.004413\n",
            "Train Epoch: 33 [29184/60000 (49%)]\tLoss: 0.012652\n",
            "Train Epoch: 33 [34048/60000 (57%)]\tLoss: 0.009905\n",
            "Train Epoch: 33 [38912/60000 (65%)]\tLoss: 0.024732\n",
            "Train Epoch: 33 [43776/60000 (73%)]\tLoss: 0.003295\n",
            "Train Epoch: 33 [48640/60000 (81%)]\tLoss: 0.007726\n",
            "Train Epoch: 33 [53504/60000 (89%)]\tLoss: 0.053095\n",
            "Train Epoch: 33 [58368/60000 (97%)]\tLoss: 0.001825\n",
            "\n",
            "Test set: Average loss: 0.4849, Accuracy: 9139/10000 (91%)\n",
            "\n",
            "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.006173\n",
            "Train Epoch: 34 [4864/60000 (8%)]\tLoss: 0.001733\n",
            "Train Epoch: 34 [9728/60000 (16%)]\tLoss: 0.007300\n",
            "Train Epoch: 34 [14592/60000 (24%)]\tLoss: 0.002376\n",
            "Train Epoch: 34 [19456/60000 (32%)]\tLoss: 0.000980\n",
            "Train Epoch: 34 [24320/60000 (40%)]\tLoss: 0.002366\n",
            "Train Epoch: 34 [29184/60000 (49%)]\tLoss: 0.003629\n",
            "Train Epoch: 34 [34048/60000 (57%)]\tLoss: 0.003992\n",
            "Train Epoch: 34 [38912/60000 (65%)]\tLoss: 0.010137\n",
            "Train Epoch: 34 [43776/60000 (73%)]\tLoss: 0.015843\n",
            "Train Epoch: 34 [48640/60000 (81%)]\tLoss: 0.014238\n",
            "Train Epoch: 34 [53504/60000 (89%)]\tLoss: 0.006051\n",
            "Train Epoch: 34 [58368/60000 (97%)]\tLoss: 0.016924\n",
            "\n",
            "Test set: Average loss: 0.5372, Accuracy: 9112/10000 (91%)\n",
            "\n",
            "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.003108\n",
            "Train Epoch: 35 [4864/60000 (8%)]\tLoss: 0.011474\n",
            "Train Epoch: 35 [9728/60000 (16%)]\tLoss: 0.011088\n",
            "Train Epoch: 35 [14592/60000 (24%)]\tLoss: 0.008707\n",
            "Train Epoch: 35 [19456/60000 (32%)]\tLoss: 0.025960\n",
            "Train Epoch: 35 [24320/60000 (40%)]\tLoss: 0.003774\n",
            "Train Epoch: 35 [29184/60000 (49%)]\tLoss: 0.008683\n",
            "Train Epoch: 35 [34048/60000 (57%)]\tLoss: 0.009535\n",
            "Train Epoch: 35 [38912/60000 (65%)]\tLoss: 0.009989\n",
            "Train Epoch: 35 [43776/60000 (73%)]\tLoss: 0.047469\n",
            "Train Epoch: 35 [48640/60000 (81%)]\tLoss: 0.010430\n",
            "Train Epoch: 35 [53504/60000 (89%)]\tLoss: 0.042483\n",
            "Train Epoch: 35 [58368/60000 (97%)]\tLoss: 0.010964\n",
            "\n",
            "Test set: Average loss: 0.5394, Accuracy: 9059/10000 (91%)\n",
            "\n",
            "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.013438\n",
            "Train Epoch: 36 [4864/60000 (8%)]\tLoss: 0.015437\n",
            "Train Epoch: 36 [9728/60000 (16%)]\tLoss: 0.036540\n",
            "Train Epoch: 36 [14592/60000 (24%)]\tLoss: 0.052605\n",
            "Train Epoch: 36 [19456/60000 (32%)]\tLoss: 0.007967\n",
            "Train Epoch: 36 [24320/60000 (40%)]\tLoss: 0.011980\n",
            "Train Epoch: 36 [29184/60000 (49%)]\tLoss: 0.002499\n",
            "Train Epoch: 36 [34048/60000 (57%)]\tLoss: 0.027387\n",
            "Train Epoch: 36 [38912/60000 (65%)]\tLoss: 0.015051\n",
            "Train Epoch: 36 [43776/60000 (73%)]\tLoss: 0.005445\n",
            "Train Epoch: 36 [48640/60000 (81%)]\tLoss: 0.018769\n",
            "Train Epoch: 36 [53504/60000 (89%)]\tLoss: 0.038585\n",
            "Train Epoch: 36 [58368/60000 (97%)]\tLoss: 0.013620\n",
            "\n",
            "Test set: Average loss: 0.5149, Accuracy: 9113/10000 (91%)\n",
            "\n",
            "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.039926\n",
            "Train Epoch: 37 [4864/60000 (8%)]\tLoss: 0.021649\n",
            "Train Epoch: 37 [9728/60000 (16%)]\tLoss: 0.027845\n",
            "Train Epoch: 37 [14592/60000 (24%)]\tLoss: 0.011397\n",
            "Train Epoch: 37 [19456/60000 (32%)]\tLoss: 0.003602\n",
            "Train Epoch: 37 [24320/60000 (40%)]\tLoss: 0.001907\n",
            "Train Epoch: 37 [29184/60000 (49%)]\tLoss: 0.011008\n",
            "Train Epoch: 37 [34048/60000 (57%)]\tLoss: 0.009053\n",
            "Train Epoch: 37 [38912/60000 (65%)]\tLoss: 0.018306\n",
            "Train Epoch: 37 [43776/60000 (73%)]\tLoss: 0.005450\n",
            "Train Epoch: 37 [48640/60000 (81%)]\tLoss: 0.012480\n",
            "Train Epoch: 37 [53504/60000 (89%)]\tLoss: 0.001593\n",
            "Train Epoch: 37 [58368/60000 (97%)]\tLoss: 0.022168\n",
            "\n",
            "Test set: Average loss: 0.5285, Accuracy: 9117/10000 (91%)\n",
            "\n",
            "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.022253\n",
            "Train Epoch: 38 [4864/60000 (8%)]\tLoss: 0.001210\n",
            "Train Epoch: 38 [9728/60000 (16%)]\tLoss: 0.004643\n",
            "Train Epoch: 38 [14592/60000 (24%)]\tLoss: 0.000632\n",
            "Train Epoch: 38 [19456/60000 (32%)]\tLoss: 0.000795\n",
            "Train Epoch: 38 [24320/60000 (40%)]\tLoss: 0.004741\n",
            "Train Epoch: 38 [29184/60000 (49%)]\tLoss: 0.000928\n",
            "Train Epoch: 38 [34048/60000 (57%)]\tLoss: 0.000922\n",
            "Train Epoch: 38 [38912/60000 (65%)]\tLoss: 0.000641\n",
            "Train Epoch: 38 [43776/60000 (73%)]\tLoss: 0.049210\n",
            "Train Epoch: 38 [48640/60000 (81%)]\tLoss: 0.015377\n",
            "Train Epoch: 38 [53504/60000 (89%)]\tLoss: 0.010051\n",
            "Train Epoch: 38 [58368/60000 (97%)]\tLoss: 0.011440\n",
            "\n",
            "Test set: Average loss: 0.5266, Accuracy: 9110/10000 (91%)\n",
            "\n",
            "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.006511\n",
            "Train Epoch: 39 [4864/60000 (8%)]\tLoss: 0.002056\n",
            "Train Epoch: 39 [9728/60000 (16%)]\tLoss: 0.000546\n",
            "Train Epoch: 39 [14592/60000 (24%)]\tLoss: 0.003227\n",
            "Train Epoch: 39 [19456/60000 (32%)]\tLoss: 0.001498\n",
            "Train Epoch: 39 [24320/60000 (40%)]\tLoss: 0.002485\n",
            "Train Epoch: 39 [29184/60000 (49%)]\tLoss: 0.002370\n",
            "Train Epoch: 39 [34048/60000 (57%)]\tLoss: 0.000177\n",
            "Train Epoch: 39 [38912/60000 (65%)]\tLoss: 0.020492\n",
            "Train Epoch: 39 [43776/60000 (73%)]\tLoss: 0.002782\n",
            "Train Epoch: 39 [48640/60000 (81%)]\tLoss: 0.022726\n",
            "Train Epoch: 39 [53504/60000 (89%)]\tLoss: 0.003768\n",
            "Train Epoch: 39 [58368/60000 (97%)]\tLoss: 0.001258\n",
            "\n",
            "Test set: Average loss: 0.5541, Accuracy: 9162/10000 (92%)\n",
            "\n",
            "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.001812\n",
            "Train Epoch: 40 [4864/60000 (8%)]\tLoss: 0.000636\n",
            "Train Epoch: 40 [9728/60000 (16%)]\tLoss: 0.041166\n",
            "Train Epoch: 40 [14592/60000 (24%)]\tLoss: 0.000992\n",
            "Train Epoch: 40 [19456/60000 (32%)]\tLoss: 0.001026\n",
            "Train Epoch: 40 [24320/60000 (40%)]\tLoss: 0.000410\n",
            "Train Epoch: 40 [29184/60000 (49%)]\tLoss: 0.014654\n",
            "Train Epoch: 40 [34048/60000 (57%)]\tLoss: 0.002086\n",
            "Train Epoch: 40 [38912/60000 (65%)]\tLoss: 0.005392\n",
            "Train Epoch: 40 [43776/60000 (73%)]\tLoss: 0.001242\n",
            "Train Epoch: 40 [48640/60000 (81%)]\tLoss: 0.009991\n",
            "Train Epoch: 40 [53504/60000 (89%)]\tLoss: 0.010258\n",
            "Train Epoch: 40 [58368/60000 (97%)]\tLoss: 0.008689\n",
            "\n",
            "Test set: Average loss: 0.5728, Accuracy: 9106/10000 (91%)\n",
            "\n",
            "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.005984\n",
            "Train Epoch: 41 [4864/60000 (8%)]\tLoss: 0.004119\n",
            "Train Epoch: 41 [9728/60000 (16%)]\tLoss: 0.002715\n",
            "Train Epoch: 41 [14592/60000 (24%)]\tLoss: 0.007189\n",
            "Train Epoch: 41 [19456/60000 (32%)]\tLoss: 0.003755\n",
            "Train Epoch: 41 [24320/60000 (40%)]\tLoss: 0.005132\n",
            "Train Epoch: 41 [29184/60000 (49%)]\tLoss: 0.011187\n",
            "Train Epoch: 41 [34048/60000 (57%)]\tLoss: 0.002497\n",
            "Train Epoch: 41 [38912/60000 (65%)]\tLoss: 0.006020\n",
            "Train Epoch: 41 [43776/60000 (73%)]\tLoss: 0.000943\n",
            "Train Epoch: 41 [48640/60000 (81%)]\tLoss: 0.001493\n",
            "Train Epoch: 41 [53504/60000 (89%)]\tLoss: 0.003375\n",
            "Train Epoch: 41 [58368/60000 (97%)]\tLoss: 0.002859\n",
            "\n",
            "Test set: Average loss: 0.5530, Accuracy: 9124/10000 (91%)\n",
            "\n",
            "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.001773\n",
            "Train Epoch: 42 [4864/60000 (8%)]\tLoss: 0.000394\n",
            "Train Epoch: 42 [9728/60000 (16%)]\tLoss: 0.000185\n",
            "Train Epoch: 42 [14592/60000 (24%)]\tLoss: 0.005570\n",
            "Train Epoch: 42 [19456/60000 (32%)]\tLoss: 0.000463\n",
            "Train Epoch: 42 [24320/60000 (40%)]\tLoss: 0.000778\n",
            "Train Epoch: 42 [29184/60000 (49%)]\tLoss: 0.000317\n",
            "Train Epoch: 42 [34048/60000 (57%)]\tLoss: 0.001620\n",
            "Train Epoch: 42 [38912/60000 (65%)]\tLoss: 0.000325\n",
            "Train Epoch: 42 [43776/60000 (73%)]\tLoss: 0.005358\n",
            "Train Epoch: 42 [48640/60000 (81%)]\tLoss: 0.015303\n",
            "Train Epoch: 42 [53504/60000 (89%)]\tLoss: 0.005695\n",
            "Train Epoch: 42 [58368/60000 (97%)]\tLoss: 0.001975\n",
            "\n",
            "Test set: Average loss: 0.5876, Accuracy: 9131/10000 (91%)\n",
            "\n",
            "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.003315\n",
            "Train Epoch: 43 [4864/60000 (8%)]\tLoss: 0.001208\n",
            "Train Epoch: 43 [9728/60000 (16%)]\tLoss: 0.008263\n",
            "Train Epoch: 43 [14592/60000 (24%)]\tLoss: 0.024516\n",
            "Train Epoch: 43 [19456/60000 (32%)]\tLoss: 0.001003\n",
            "Train Epoch: 43 [24320/60000 (40%)]\tLoss: 0.001725\n",
            "Train Epoch: 43 [29184/60000 (49%)]\tLoss: 0.003872\n",
            "Train Epoch: 43 [34048/60000 (57%)]\tLoss: 0.008369\n",
            "Train Epoch: 43 [38912/60000 (65%)]\tLoss: 0.024503\n",
            "Train Epoch: 43 [43776/60000 (73%)]\tLoss: 0.013307\n",
            "Train Epoch: 43 [48640/60000 (81%)]\tLoss: 0.005049\n",
            "Train Epoch: 43 [53504/60000 (89%)]\tLoss: 0.001231\n",
            "Train Epoch: 43 [58368/60000 (97%)]\tLoss: 0.034664\n",
            "\n",
            "Test set: Average loss: 0.5809, Accuracy: 9112/10000 (91%)\n",
            "\n",
            "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.026442\n",
            "Train Epoch: 44 [4864/60000 (8%)]\tLoss: 0.002197\n",
            "Train Epoch: 44 [9728/60000 (16%)]\tLoss: 0.034440\n",
            "Train Epoch: 44 [14592/60000 (24%)]\tLoss: 0.000326\n",
            "Train Epoch: 44 [19456/60000 (32%)]\tLoss: 0.001925\n",
            "Train Epoch: 44 [24320/60000 (40%)]\tLoss: 0.002581\n",
            "Train Epoch: 44 [29184/60000 (49%)]\tLoss: 0.001481\n",
            "Train Epoch: 44 [34048/60000 (57%)]\tLoss: 0.054778\n",
            "Train Epoch: 44 [38912/60000 (65%)]\tLoss: 0.008955\n",
            "Train Epoch: 44 [43776/60000 (73%)]\tLoss: 0.009840\n",
            "Train Epoch: 44 [48640/60000 (81%)]\tLoss: 0.017944\n",
            "Train Epoch: 44 [53504/60000 (89%)]\tLoss: 0.000651\n",
            "Train Epoch: 44 [58368/60000 (97%)]\tLoss: 0.016652\n",
            "\n",
            "Test set: Average loss: 0.5580, Accuracy: 9121/10000 (91%)\n",
            "\n",
            "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.003482\n",
            "Train Epoch: 45 [4864/60000 (8%)]\tLoss: 0.000617\n",
            "Train Epoch: 45 [9728/60000 (16%)]\tLoss: 0.000925\n",
            "Train Epoch: 45 [14592/60000 (24%)]\tLoss: 0.000662\n",
            "Train Epoch: 45 [19456/60000 (32%)]\tLoss: 0.017733\n",
            "Train Epoch: 45 [24320/60000 (40%)]\tLoss: 0.005347\n",
            "Train Epoch: 45 [29184/60000 (49%)]\tLoss: 0.001873\n",
            "Train Epoch: 45 [34048/60000 (57%)]\tLoss: 0.002800\n",
            "Train Epoch: 45 [38912/60000 (65%)]\tLoss: 0.007841\n",
            "Train Epoch: 45 [43776/60000 (73%)]\tLoss: 0.011524\n",
            "Train Epoch: 45 [48640/60000 (81%)]\tLoss: 0.017365\n",
            "Train Epoch: 45 [53504/60000 (89%)]\tLoss: 0.004840\n",
            "Train Epoch: 45 [58368/60000 (97%)]\tLoss: 0.024628\n",
            "\n",
            "Test set: Average loss: 0.5736, Accuracy: 9118/10000 (91%)\n",
            "\n",
            "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.045636\n",
            "Train Epoch: 46 [4864/60000 (8%)]\tLoss: 0.022762\n",
            "Train Epoch: 46 [9728/60000 (16%)]\tLoss: 0.027795\n",
            "Train Epoch: 46 [14592/60000 (24%)]\tLoss: 0.006027\n",
            "Train Epoch: 46 [19456/60000 (32%)]\tLoss: 0.000725\n",
            "Train Epoch: 46 [24320/60000 (40%)]\tLoss: 0.001834\n",
            "Train Epoch: 46 [29184/60000 (49%)]\tLoss: 0.000242\n",
            "Train Epoch: 46 [34048/60000 (57%)]\tLoss: 0.003321\n",
            "Train Epoch: 46 [38912/60000 (65%)]\tLoss: 0.000906\n",
            "Train Epoch: 46 [43776/60000 (73%)]\tLoss: 0.000743\n",
            "Train Epoch: 46 [48640/60000 (81%)]\tLoss: 0.002288\n",
            "Train Epoch: 46 [53504/60000 (89%)]\tLoss: 0.003319\n",
            "Train Epoch: 46 [58368/60000 (97%)]\tLoss: 0.002235\n",
            "\n",
            "Test set: Average loss: 0.5729, Accuracy: 9097/10000 (91%)\n",
            "\n",
            "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.000825\n",
            "Train Epoch: 47 [4864/60000 (8%)]\tLoss: 0.000574\n",
            "Train Epoch: 47 [9728/60000 (16%)]\tLoss: 0.000750\n",
            "Train Epoch: 47 [14592/60000 (24%)]\tLoss: 0.000767\n",
            "Train Epoch: 47 [19456/60000 (32%)]\tLoss: 0.000311\n",
            "Train Epoch: 47 [24320/60000 (40%)]\tLoss: 0.004773\n",
            "Train Epoch: 47 [29184/60000 (49%)]\tLoss: 0.000622\n",
            "Train Epoch: 47 [34048/60000 (57%)]\tLoss: 0.008014\n",
            "Train Epoch: 47 [38912/60000 (65%)]\tLoss: 0.001632\n",
            "Train Epoch: 47 [43776/60000 (73%)]\tLoss: 0.000332\n",
            "Train Epoch: 47 [48640/60000 (81%)]\tLoss: 0.000595\n",
            "Train Epoch: 47 [53504/60000 (89%)]\tLoss: 0.008347\n",
            "Train Epoch: 47 [58368/60000 (97%)]\tLoss: 0.001198\n",
            "\n",
            "Test set: Average loss: 0.6074, Accuracy: 9141/10000 (91%)\n",
            "\n",
            "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.000211\n",
            "Train Epoch: 48 [4864/60000 (8%)]\tLoss: 0.001209\n",
            "Train Epoch: 48 [9728/60000 (16%)]\tLoss: 0.000375\n",
            "Train Epoch: 48 [14592/60000 (24%)]\tLoss: 0.000932\n",
            "Train Epoch: 48 [19456/60000 (32%)]\tLoss: 0.001218\n",
            "Train Epoch: 48 [24320/60000 (40%)]\tLoss: 0.000165\n",
            "Train Epoch: 48 [29184/60000 (49%)]\tLoss: 0.000487\n",
            "Train Epoch: 48 [34048/60000 (57%)]\tLoss: 0.001687\n",
            "Train Epoch: 48 [38912/60000 (65%)]\tLoss: 0.001277\n",
            "Train Epoch: 48 [43776/60000 (73%)]\tLoss: 0.000100\n",
            "Train Epoch: 48 [48640/60000 (81%)]\tLoss: 0.000177\n",
            "Train Epoch: 48 [53504/60000 (89%)]\tLoss: 0.000070\n",
            "Train Epoch: 48 [58368/60000 (97%)]\tLoss: 0.002219\n",
            "\n",
            "Test set: Average loss: 0.5955, Accuracy: 9153/10000 (92%)\n",
            "\n",
            "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.000232\n",
            "Train Epoch: 49 [4864/60000 (8%)]\tLoss: 0.000068\n",
            "Train Epoch: 49 [9728/60000 (16%)]\tLoss: 0.000205\n",
            "Train Epoch: 49 [14592/60000 (24%)]\tLoss: 0.010330\n",
            "Train Epoch: 49 [19456/60000 (32%)]\tLoss: 0.001630\n",
            "Train Epoch: 49 [24320/60000 (40%)]\tLoss: 0.000276\n",
            "Train Epoch: 49 [29184/60000 (49%)]\tLoss: 0.000483\n",
            "Train Epoch: 49 [34048/60000 (57%)]\tLoss: 0.004949\n",
            "Train Epoch: 49 [38912/60000 (65%)]\tLoss: 0.000374\n",
            "Train Epoch: 49 [43776/60000 (73%)]\tLoss: 0.000540\n",
            "Train Epoch: 49 [48640/60000 (81%)]\tLoss: 0.000312\n",
            "Train Epoch: 49 [53504/60000 (89%)]\tLoss: 0.000425\n",
            "Train Epoch: 49 [58368/60000 (97%)]\tLoss: 0.001620\n",
            "\n",
            "Test set: Average loss: 0.5907, Accuracy: 9146/10000 (91%)\n",
            "\n",
            "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.000215\n",
            "Train Epoch: 50 [4864/60000 (8%)]\tLoss: 0.000196\n",
            "Train Epoch: 50 [9728/60000 (16%)]\tLoss: 0.000457\n",
            "Train Epoch: 50 [14592/60000 (24%)]\tLoss: 0.000549\n",
            "Train Epoch: 50 [19456/60000 (32%)]\tLoss: 0.001510\n",
            "Train Epoch: 50 [24320/60000 (40%)]\tLoss: 0.007853\n",
            "Train Epoch: 50 [29184/60000 (49%)]\tLoss: 0.034635\n",
            "Train Epoch: 50 [34048/60000 (57%)]\tLoss: 0.002404\n",
            "Train Epoch: 50 [38912/60000 (65%)]\tLoss: 0.000244\n",
            "Train Epoch: 50 [43776/60000 (73%)]\tLoss: 0.000615\n",
            "Train Epoch: 50 [48640/60000 (81%)]\tLoss: 0.000768\n",
            "Train Epoch: 50 [53504/60000 (89%)]\tLoss: 0.000153\n",
            "Train Epoch: 50 [58368/60000 (97%)]\tLoss: 0.000159\n",
            "\n",
            "Test set: Average loss: 0.6159, Accuracy: 9146/10000 (91%)\n",
            "\n",
            "Train Epoch: 51 [0/60000 (0%)]\tLoss: 0.000387\n",
            "Train Epoch: 51 [4864/60000 (8%)]\tLoss: 0.000653\n",
            "Train Epoch: 51 [9728/60000 (16%)]\tLoss: 0.000208\n",
            "Train Epoch: 51 [14592/60000 (24%)]\tLoss: 0.000795\n",
            "Train Epoch: 51 [19456/60000 (32%)]\tLoss: 0.000278\n",
            "Train Epoch: 51 [24320/60000 (40%)]\tLoss: 0.000215\n",
            "Train Epoch: 51 [29184/60000 (49%)]\tLoss: 0.000069\n",
            "Train Epoch: 51 [34048/60000 (57%)]\tLoss: 0.001110\n",
            "Train Epoch: 51 [38912/60000 (65%)]\tLoss: 0.001683\n",
            "Train Epoch: 51 [43776/60000 (73%)]\tLoss: 0.000130\n",
            "Train Epoch: 51 [48640/60000 (81%)]\tLoss: 0.000075\n",
            "Train Epoch: 51 [53504/60000 (89%)]\tLoss: 0.000098\n",
            "Train Epoch: 51 [58368/60000 (97%)]\tLoss: 0.000565\n",
            "\n",
            "Test set: Average loss: 0.6190, Accuracy: 9167/10000 (92%)\n",
            "\n",
            "Train Epoch: 52 [0/60000 (0%)]\tLoss: 0.000032\n",
            "Train Epoch: 52 [4864/60000 (8%)]\tLoss: 0.000062\n",
            "Train Epoch: 52 [9728/60000 (16%)]\tLoss: 0.000140\n",
            "Train Epoch: 52 [14592/60000 (24%)]\tLoss: 0.000365\n",
            "Train Epoch: 52 [19456/60000 (32%)]\tLoss: 0.000031\n",
            "Train Epoch: 52 [24320/60000 (40%)]\tLoss: 0.000822\n",
            "Train Epoch: 52 [29184/60000 (49%)]\tLoss: 0.000196\n",
            "Train Epoch: 52 [34048/60000 (57%)]\tLoss: 0.000046\n",
            "Train Epoch: 52 [38912/60000 (65%)]\tLoss: 0.000019\n",
            "Train Epoch: 52 [43776/60000 (73%)]\tLoss: 0.000401\n",
            "Train Epoch: 52 [48640/60000 (81%)]\tLoss: 0.000090\n",
            "Train Epoch: 52 [53504/60000 (89%)]\tLoss: 0.000151\n",
            "Train Epoch: 52 [58368/60000 (97%)]\tLoss: 0.007065\n",
            "\n",
            "Test set: Average loss: 0.6342, Accuracy: 9146/10000 (91%)\n",
            "\n",
            "Train Epoch: 53 [0/60000 (0%)]\tLoss: 0.000043\n",
            "Train Epoch: 53 [4864/60000 (8%)]\tLoss: 0.005069\n",
            "Train Epoch: 53 [9728/60000 (16%)]\tLoss: 0.000301\n",
            "Train Epoch: 53 [14592/60000 (24%)]\tLoss: 0.000713\n",
            "Train Epoch: 53 [19456/60000 (32%)]\tLoss: 0.000082\n",
            "Train Epoch: 53 [24320/60000 (40%)]\tLoss: 0.000071\n",
            "Train Epoch: 53 [29184/60000 (49%)]\tLoss: 0.000507\n",
            "Train Epoch: 53 [34048/60000 (57%)]\tLoss: 0.000133\n",
            "Train Epoch: 53 [38912/60000 (65%)]\tLoss: 0.000087\n",
            "Train Epoch: 53 [43776/60000 (73%)]\tLoss: 0.000043\n",
            "Train Epoch: 53 [48640/60000 (81%)]\tLoss: 0.000088\n",
            "Train Epoch: 53 [53504/60000 (89%)]\tLoss: 0.000075\n",
            "Train Epoch: 53 [58368/60000 (97%)]\tLoss: 0.000034\n",
            "\n",
            "Test set: Average loss: 0.6410, Accuracy: 9150/10000 (92%)\n",
            "\n",
            "Train Epoch: 54 [0/60000 (0%)]\tLoss: 0.000710\n",
            "Train Epoch: 54 [4864/60000 (8%)]\tLoss: 0.000109\n",
            "Train Epoch: 54 [9728/60000 (16%)]\tLoss: 0.000056\n",
            "Train Epoch: 54 [14592/60000 (24%)]\tLoss: 0.000122\n",
            "Train Epoch: 54 [19456/60000 (32%)]\tLoss: 0.000044\n",
            "Train Epoch: 54 [24320/60000 (40%)]\tLoss: 0.000042\n",
            "Train Epoch: 54 [29184/60000 (49%)]\tLoss: 0.000160\n",
            "Train Epoch: 54 [34048/60000 (57%)]\tLoss: 0.000071\n",
            "Train Epoch: 54 [38912/60000 (65%)]\tLoss: 0.000033\n",
            "Train Epoch: 54 [43776/60000 (73%)]\tLoss: 0.000070\n",
            "Train Epoch: 54 [48640/60000 (81%)]\tLoss: 0.000166\n",
            "Train Epoch: 54 [53504/60000 (89%)]\tLoss: 0.000774\n",
            "Train Epoch: 54 [58368/60000 (97%)]\tLoss: 0.000055\n",
            "\n",
            "Test set: Average loss: 0.6428, Accuracy: 9169/10000 (92%)\n",
            "\n",
            "Train Epoch: 55 [0/60000 (0%)]\tLoss: 0.000053\n",
            "Train Epoch: 55 [4864/60000 (8%)]\tLoss: 0.000561\n",
            "Train Epoch: 55 [9728/60000 (16%)]\tLoss: 0.000016\n",
            "Train Epoch: 55 [14592/60000 (24%)]\tLoss: 0.000043\n",
            "Train Epoch: 55 [19456/60000 (32%)]\tLoss: 0.000118\n",
            "Train Epoch: 55 [24320/60000 (40%)]\tLoss: 0.000142\n",
            "Train Epoch: 55 [29184/60000 (49%)]\tLoss: 0.000170\n",
            "Train Epoch: 55 [34048/60000 (57%)]\tLoss: 0.000029\n",
            "Train Epoch: 55 [38912/60000 (65%)]\tLoss: 0.000019\n",
            "Train Epoch: 55 [43776/60000 (73%)]\tLoss: 0.000182\n",
            "Train Epoch: 55 [48640/60000 (81%)]\tLoss: 0.000103\n",
            "Train Epoch: 55 [53504/60000 (89%)]\tLoss: 0.000031\n",
            "Train Epoch: 55 [58368/60000 (97%)]\tLoss: 0.000034\n",
            "\n",
            "Test set: Average loss: 0.6453, Accuracy: 9161/10000 (92%)\n",
            "\n",
            "Train Epoch: 56 [0/60000 (0%)]\tLoss: 0.000078\n",
            "Train Epoch: 56 [4864/60000 (8%)]\tLoss: 0.000079\n",
            "Train Epoch: 56 [9728/60000 (16%)]\tLoss: 0.000020\n",
            "Train Epoch: 56 [14592/60000 (24%)]\tLoss: 0.000040\n",
            "Train Epoch: 56 [19456/60000 (32%)]\tLoss: 0.000045\n",
            "Train Epoch: 56 [24320/60000 (40%)]\tLoss: 0.000014\n",
            "Train Epoch: 56 [29184/60000 (49%)]\tLoss: 0.000011\n",
            "Train Epoch: 56 [34048/60000 (57%)]\tLoss: 0.000093\n",
            "Train Epoch: 56 [38912/60000 (65%)]\tLoss: 0.000021\n",
            "Train Epoch: 56 [43776/60000 (73%)]\tLoss: 0.000028\n",
            "Train Epoch: 56 [48640/60000 (81%)]\tLoss: 0.000021\n",
            "Train Epoch: 56 [53504/60000 (89%)]\tLoss: 0.000067\n",
            "Train Epoch: 56 [58368/60000 (97%)]\tLoss: 0.000030\n",
            "\n",
            "Test set: Average loss: 0.6467, Accuracy: 9182/10000 (92%)\n",
            "\n",
            "Train Epoch: 57 [0/60000 (0%)]\tLoss: 0.000046\n",
            "Train Epoch: 57 [4864/60000 (8%)]\tLoss: 0.000147\n",
            "Train Epoch: 57 [9728/60000 (16%)]\tLoss: 0.000086\n",
            "Train Epoch: 57 [14592/60000 (24%)]\tLoss: 0.000015\n",
            "Train Epoch: 57 [19456/60000 (32%)]\tLoss: 0.000072\n",
            "Train Epoch: 57 [24320/60000 (40%)]\tLoss: 0.000021\n",
            "Train Epoch: 57 [29184/60000 (49%)]\tLoss: 0.000025\n",
            "Train Epoch: 57 [34048/60000 (57%)]\tLoss: 0.000086\n",
            "Train Epoch: 57 [38912/60000 (65%)]\tLoss: 0.000011\n",
            "Train Epoch: 57 [43776/60000 (73%)]\tLoss: 0.000067\n",
            "Train Epoch: 57 [48640/60000 (81%)]\tLoss: 0.000077\n",
            "Train Epoch: 57 [53504/60000 (89%)]\tLoss: 0.000037\n",
            "Train Epoch: 57 [58368/60000 (97%)]\tLoss: 0.000018\n",
            "\n",
            "Test set: Average loss: 0.6403, Accuracy: 9181/10000 (92%)\n",
            "\n",
            "Train Epoch: 58 [0/60000 (0%)]\tLoss: 0.000066\n",
            "Train Epoch: 58 [4864/60000 (8%)]\tLoss: 0.000014\n",
            "Train Epoch: 58 [9728/60000 (16%)]\tLoss: 0.000057\n",
            "Train Epoch: 58 [14592/60000 (24%)]\tLoss: 0.000020\n",
            "Train Epoch: 58 [19456/60000 (32%)]\tLoss: 0.000019\n",
            "Train Epoch: 58 [24320/60000 (40%)]\tLoss: 0.000049\n",
            "Train Epoch: 58 [29184/60000 (49%)]\tLoss: 0.000019\n",
            "Train Epoch: 58 [34048/60000 (57%)]\tLoss: 0.000125\n",
            "Train Epoch: 58 [38912/60000 (65%)]\tLoss: 0.000011\n",
            "Train Epoch: 58 [43776/60000 (73%)]\tLoss: 0.000013\n",
            "Train Epoch: 58 [48640/60000 (81%)]\tLoss: 0.000086\n",
            "Train Epoch: 58 [53504/60000 (89%)]\tLoss: 0.000048\n",
            "Train Epoch: 58 [58368/60000 (97%)]\tLoss: 0.000011\n",
            "\n",
            "Test set: Average loss: 0.6497, Accuracy: 9172/10000 (92%)\n",
            "\n",
            "Train Epoch: 59 [0/60000 (0%)]\tLoss: 0.000050\n",
            "Train Epoch: 59 [4864/60000 (8%)]\tLoss: 0.000043\n",
            "Train Epoch: 59 [9728/60000 (16%)]\tLoss: 0.000009\n",
            "Train Epoch: 59 [14592/60000 (24%)]\tLoss: 0.000063\n",
            "Train Epoch: 59 [19456/60000 (32%)]\tLoss: 0.000021\n",
            "Train Epoch: 59 [24320/60000 (40%)]\tLoss: 0.000038\n",
            "Train Epoch: 59 [29184/60000 (49%)]\tLoss: 0.000023\n",
            "Train Epoch: 59 [34048/60000 (57%)]\tLoss: 0.000009\n",
            "Train Epoch: 59 [38912/60000 (65%)]\tLoss: 0.000093\n",
            "Train Epoch: 59 [43776/60000 (73%)]\tLoss: 0.000018\n",
            "Train Epoch: 59 [48640/60000 (81%)]\tLoss: 0.000070\n",
            "Train Epoch: 59 [53504/60000 (89%)]\tLoss: 0.000025\n",
            "Train Epoch: 59 [58368/60000 (97%)]\tLoss: 0.000017\n",
            "\n",
            "Test set: Average loss: 0.6432, Accuracy: 9177/10000 (92%)\n",
            "\n",
            "Train Epoch: 60 [0/60000 (0%)]\tLoss: 0.000019\n",
            "Train Epoch: 60 [4864/60000 (8%)]\tLoss: 0.000035\n",
            "Train Epoch: 60 [9728/60000 (16%)]\tLoss: 0.000028\n",
            "Train Epoch: 60 [14592/60000 (24%)]\tLoss: 0.000054\n",
            "Train Epoch: 60 [19456/60000 (32%)]\tLoss: 0.000009\n",
            "Train Epoch: 60 [24320/60000 (40%)]\tLoss: 0.000023\n",
            "Train Epoch: 60 [29184/60000 (49%)]\tLoss: 0.000027\n",
            "Train Epoch: 60 [34048/60000 (57%)]\tLoss: 0.000033\n",
            "Train Epoch: 60 [38912/60000 (65%)]\tLoss: 0.000019\n",
            "Train Epoch: 60 [43776/60000 (73%)]\tLoss: 0.000057\n",
            "Train Epoch: 60 [48640/60000 (81%)]\tLoss: 0.000023\n",
            "Train Epoch: 60 [53504/60000 (89%)]\tLoss: 0.000016\n",
            "Train Epoch: 60 [58368/60000 (97%)]\tLoss: 0.000023\n",
            "\n",
            "Test set: Average loss: 0.6500, Accuracy: 9171/10000 (92%)\n",
            "\n",
            "Train Epoch: 61 [0/60000 (0%)]\tLoss: 0.000029\n",
            "Train Epoch: 61 [4864/60000 (8%)]\tLoss: 0.000022\n",
            "Train Epoch: 61 [9728/60000 (16%)]\tLoss: 0.000072\n",
            "Train Epoch: 61 [14592/60000 (24%)]\tLoss: 0.000009\n",
            "Train Epoch: 61 [19456/60000 (32%)]\tLoss: 0.000007\n",
            "Train Epoch: 61 [24320/60000 (40%)]\tLoss: 0.000021\n",
            "Train Epoch: 61 [29184/60000 (49%)]\tLoss: 0.000018\n",
            "Train Epoch: 61 [34048/60000 (57%)]\tLoss: 0.000009\n",
            "Train Epoch: 61 [38912/60000 (65%)]\tLoss: 0.000010\n",
            "Train Epoch: 61 [43776/60000 (73%)]\tLoss: 0.000013\n",
            "Train Epoch: 61 [48640/60000 (81%)]\tLoss: 0.000015\n",
            "Train Epoch: 61 [53504/60000 (89%)]\tLoss: 0.000012\n",
            "Train Epoch: 61 [58368/60000 (97%)]\tLoss: 0.000023\n",
            "\n",
            "Test set: Average loss: 0.6477, Accuracy: 9174/10000 (92%)\n",
            "\n",
            "Train Epoch: 62 [0/60000 (0%)]\tLoss: 0.000017\n",
            "Train Epoch: 62 [4864/60000 (8%)]\tLoss: 0.000019\n",
            "Train Epoch: 62 [9728/60000 (16%)]\tLoss: 0.000014\n",
            "Train Epoch: 62 [14592/60000 (24%)]\tLoss: 0.000020\n",
            "Train Epoch: 62 [19456/60000 (32%)]\tLoss: 0.000016\n",
            "Train Epoch: 62 [24320/60000 (40%)]\tLoss: 0.000061\n",
            "Train Epoch: 62 [29184/60000 (49%)]\tLoss: 0.000027\n",
            "Train Epoch: 62 [34048/60000 (57%)]\tLoss: 0.000024\n",
            "Train Epoch: 62 [38912/60000 (65%)]\tLoss: 0.000016\n",
            "Train Epoch: 62 [43776/60000 (73%)]\tLoss: 0.000015\n",
            "Train Epoch: 62 [48640/60000 (81%)]\tLoss: 0.000012\n",
            "Train Epoch: 62 [53504/60000 (89%)]\tLoss: 0.000014\n",
            "Train Epoch: 62 [58368/60000 (97%)]\tLoss: 0.000025\n",
            "\n",
            "Test set: Average loss: 0.6493, Accuracy: 9175/10000 (92%)\n",
            "\n",
            "Train Epoch: 63 [0/60000 (0%)]\tLoss: 0.000010\n",
            "Train Epoch: 63 [4864/60000 (8%)]\tLoss: 0.000048\n",
            "Train Epoch: 63 [9728/60000 (16%)]\tLoss: 0.000014\n",
            "Train Epoch: 63 [14592/60000 (24%)]\tLoss: 0.000012\n",
            "Train Epoch: 63 [19456/60000 (32%)]\tLoss: 0.000025\n",
            "Train Epoch: 63 [24320/60000 (40%)]\tLoss: 0.000017\n",
            "Train Epoch: 63 [29184/60000 (49%)]\tLoss: 0.000014\n",
            "Train Epoch: 63 [34048/60000 (57%)]\tLoss: 0.000008\n",
            "Train Epoch: 63 [38912/60000 (65%)]\tLoss: 0.000019\n",
            "Train Epoch: 63 [43776/60000 (73%)]\tLoss: 0.000010\n",
            "Train Epoch: 63 [48640/60000 (81%)]\tLoss: 0.000020\n",
            "Train Epoch: 63 [53504/60000 (89%)]\tLoss: 0.000032\n",
            "Train Epoch: 63 [58368/60000 (97%)]\tLoss: 0.000018\n",
            "\n",
            "Test set: Average loss: 0.6504, Accuracy: 9186/10000 (92%)\n",
            "\n",
            "Train Epoch: 64 [0/60000 (0%)]\tLoss: 0.000021\n",
            "Train Epoch: 64 [4864/60000 (8%)]\tLoss: 0.000017\n",
            "Train Epoch: 64 [9728/60000 (16%)]\tLoss: 0.000023\n",
            "Train Epoch: 64 [14592/60000 (24%)]\tLoss: 0.000031\n",
            "Train Epoch: 64 [19456/60000 (32%)]\tLoss: 0.000008\n",
            "Train Epoch: 64 [24320/60000 (40%)]\tLoss: 0.000023\n",
            "Train Epoch: 64 [29184/60000 (49%)]\tLoss: 0.000008\n",
            "Train Epoch: 64 [34048/60000 (57%)]\tLoss: 0.000034\n",
            "Train Epoch: 64 [38912/60000 (65%)]\tLoss: 0.000027\n",
            "Train Epoch: 64 [43776/60000 (73%)]\tLoss: 0.000007\n",
            "Train Epoch: 64 [48640/60000 (81%)]\tLoss: 0.000079\n",
            "Train Epoch: 64 [53504/60000 (89%)]\tLoss: 0.000029\n",
            "Train Epoch: 64 [58368/60000 (97%)]\tLoss: 0.000020\n",
            "\n",
            "Test set: Average loss: 0.6466, Accuracy: 9180/10000 (92%)\n",
            "\n",
            "Train Epoch: 65 [0/60000 (0%)]\tLoss: 0.000012\n",
            "Train Epoch: 65 [4864/60000 (8%)]\tLoss: 0.000017\n",
            "Train Epoch: 65 [9728/60000 (16%)]\tLoss: 0.000038\n",
            "Train Epoch: 65 [14592/60000 (24%)]\tLoss: 0.000011\n",
            "Train Epoch: 65 [19456/60000 (32%)]\tLoss: 0.000013\n",
            "Train Epoch: 65 [24320/60000 (40%)]\tLoss: 0.000005\n",
            "Train Epoch: 65 [29184/60000 (49%)]\tLoss: 0.000007\n",
            "Train Epoch: 65 [34048/60000 (57%)]\tLoss: 0.000021\n",
            "Train Epoch: 65 [38912/60000 (65%)]\tLoss: 0.000015\n",
            "Train Epoch: 65 [43776/60000 (73%)]\tLoss: 0.000012\n",
            "Train Epoch: 65 [48640/60000 (81%)]\tLoss: 0.000007\n",
            "Train Epoch: 65 [53504/60000 (89%)]\tLoss: 0.000016\n",
            "Train Epoch: 65 [58368/60000 (97%)]\tLoss: 0.000008\n",
            "\n",
            "Test set: Average loss: 0.6542, Accuracy: 9181/10000 (92%)\n",
            "\n",
            "Train Epoch: 66 [0/60000 (0%)]\tLoss: 0.000015\n",
            "Train Epoch: 66 [4864/60000 (8%)]\tLoss: 0.000008\n",
            "Train Epoch: 66 [9728/60000 (16%)]\tLoss: 0.000007\n",
            "Train Epoch: 66 [14592/60000 (24%)]\tLoss: 0.000014\n",
            "Train Epoch: 66 [19456/60000 (32%)]\tLoss: 0.000011\n",
            "Train Epoch: 66 [24320/60000 (40%)]\tLoss: 0.000020\n",
            "Train Epoch: 66 [29184/60000 (49%)]\tLoss: 0.000011\n",
            "Train Epoch: 66 [34048/60000 (57%)]\tLoss: 0.000015\n",
            "Train Epoch: 66 [38912/60000 (65%)]\tLoss: 0.000027\n",
            "Train Epoch: 66 [43776/60000 (73%)]\tLoss: 0.000018\n",
            "Train Epoch: 66 [48640/60000 (81%)]\tLoss: 0.000031\n",
            "Train Epoch: 66 [53504/60000 (89%)]\tLoss: 0.000009\n",
            "Train Epoch: 66 [58368/60000 (97%)]\tLoss: 0.000043\n",
            "\n",
            "Test set: Average loss: 0.6565, Accuracy: 9184/10000 (92%)\n",
            "\n",
            "Train Epoch: 67 [0/60000 (0%)]\tLoss: 0.000011\n",
            "Train Epoch: 67 [4864/60000 (8%)]\tLoss: 0.000007\n",
            "Train Epoch: 67 [9728/60000 (16%)]\tLoss: 0.000015\n",
            "Train Epoch: 67 [14592/60000 (24%)]\tLoss: 0.000018\n",
            "Train Epoch: 67 [19456/60000 (32%)]\tLoss: 0.000017\n",
            "Train Epoch: 67 [24320/60000 (40%)]\tLoss: 0.000028\n",
            "Train Epoch: 67 [29184/60000 (49%)]\tLoss: 0.000012\n",
            "Train Epoch: 67 [34048/60000 (57%)]\tLoss: 0.000027\n",
            "Train Epoch: 67 [38912/60000 (65%)]\tLoss: 0.000011\n",
            "Train Epoch: 67 [43776/60000 (73%)]\tLoss: 0.000033\n",
            "Train Epoch: 67 [48640/60000 (81%)]\tLoss: 0.000005\n",
            "Train Epoch: 67 [53504/60000 (89%)]\tLoss: 0.000005\n",
            "Train Epoch: 67 [58368/60000 (97%)]\tLoss: 0.000012\n",
            "\n",
            "Test set: Average loss: 0.6578, Accuracy: 9183/10000 (92%)\n",
            "\n",
            "Train Epoch: 68 [0/60000 (0%)]\tLoss: 0.000011\n",
            "Train Epoch: 68 [4864/60000 (8%)]\tLoss: 0.000012\n",
            "Train Epoch: 68 [9728/60000 (16%)]\tLoss: 0.000033\n",
            "Train Epoch: 68 [14592/60000 (24%)]\tLoss: 0.000020\n",
            "Train Epoch: 68 [19456/60000 (32%)]\tLoss: 0.000023\n",
            "Train Epoch: 68 [24320/60000 (40%)]\tLoss: 0.000012\n",
            "Train Epoch: 68 [29184/60000 (49%)]\tLoss: 0.000010\n",
            "Train Epoch: 68 [34048/60000 (57%)]\tLoss: 0.000007\n",
            "Train Epoch: 68 [38912/60000 (65%)]\tLoss: 0.000008\n",
            "Train Epoch: 68 [43776/60000 (73%)]\tLoss: 0.000020\n",
            "Train Epoch: 68 [48640/60000 (81%)]\tLoss: 0.000024\n",
            "Train Epoch: 68 [53504/60000 (89%)]\tLoss: 0.000013\n",
            "Train Epoch: 68 [58368/60000 (97%)]\tLoss: 0.000012\n",
            "\n",
            "Test set: Average loss: 0.6600, Accuracy: 9176/10000 (92%)\n",
            "\n",
            "Train Epoch: 69 [0/60000 (0%)]\tLoss: 0.000010\n",
            "Train Epoch: 69 [4864/60000 (8%)]\tLoss: 0.000015\n",
            "Train Epoch: 69 [9728/60000 (16%)]\tLoss: 0.000014\n",
            "Train Epoch: 69 [14592/60000 (24%)]\tLoss: 0.000011\n",
            "Train Epoch: 69 [19456/60000 (32%)]\tLoss: 0.000016\n",
            "Train Epoch: 69 [24320/60000 (40%)]\tLoss: 0.000007\n",
            "Train Epoch: 69 [29184/60000 (49%)]\tLoss: 0.000011\n",
            "Train Epoch: 69 [34048/60000 (57%)]\tLoss: 0.000010\n",
            "Train Epoch: 69 [38912/60000 (65%)]\tLoss: 0.000012\n",
            "Train Epoch: 69 [43776/60000 (73%)]\tLoss: 0.000018\n",
            "Train Epoch: 69 [48640/60000 (81%)]\tLoss: 0.000009\n",
            "Train Epoch: 69 [53504/60000 (89%)]\tLoss: 0.000014\n",
            "Train Epoch: 69 [58368/60000 (97%)]\tLoss: 0.000015\n",
            "\n",
            "Test set: Average loss: 0.6548, Accuracy: 9176/10000 (92%)\n",
            "\n",
            "Train Epoch: 70 [0/60000 (0%)]\tLoss: 0.000017\n",
            "Train Epoch: 70 [4864/60000 (8%)]\tLoss: 0.000015\n",
            "Train Epoch: 70 [9728/60000 (16%)]\tLoss: 0.000004\n",
            "Train Epoch: 70 [14592/60000 (24%)]\tLoss: 0.000008\n",
            "Train Epoch: 70 [19456/60000 (32%)]\tLoss: 0.000049\n",
            "Train Epoch: 70 [24320/60000 (40%)]\tLoss: 0.000020\n",
            "Train Epoch: 70 [29184/60000 (49%)]\tLoss: 0.000008\n",
            "Train Epoch: 70 [34048/60000 (57%)]\tLoss: 0.000011\n",
            "Train Epoch: 70 [38912/60000 (65%)]\tLoss: 0.000022\n",
            "Train Epoch: 70 [43776/60000 (73%)]\tLoss: 0.000015\n",
            "Train Epoch: 70 [48640/60000 (81%)]\tLoss: 0.000015\n",
            "Train Epoch: 70 [53504/60000 (89%)]\tLoss: 0.000007\n",
            "Train Epoch: 70 [58368/60000 (97%)]\tLoss: 0.000014\n",
            "\n",
            "Test set: Average loss: 0.6595, Accuracy: 9168/10000 (92%)\n",
            "\n",
            "Train Epoch: 71 [0/60000 (0%)]\tLoss: 0.000007\n",
            "Train Epoch: 71 [4864/60000 (8%)]\tLoss: 0.000019\n",
            "Train Epoch: 71 [9728/60000 (16%)]\tLoss: 0.000005\n",
            "Train Epoch: 71 [14592/60000 (24%)]\tLoss: 0.000005\n",
            "Train Epoch: 71 [19456/60000 (32%)]\tLoss: 0.000025\n",
            "Train Epoch: 71 [24320/60000 (40%)]\tLoss: 0.000009\n",
            "Train Epoch: 71 [29184/60000 (49%)]\tLoss: 0.000023\n",
            "Train Epoch: 71 [34048/60000 (57%)]\tLoss: 0.000014\n",
            "Train Epoch: 71 [38912/60000 (65%)]\tLoss: 0.000026\n",
            "Train Epoch: 71 [43776/60000 (73%)]\tLoss: 0.000034\n",
            "Train Epoch: 71 [48640/60000 (81%)]\tLoss: 0.000011\n",
            "Train Epoch: 71 [53504/60000 (89%)]\tLoss: 0.000009\n",
            "Train Epoch: 71 [58368/60000 (97%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 0.6530, Accuracy: 9186/10000 (92%)\n",
            "\n",
            "Train Epoch: 72 [0/60000 (0%)]\tLoss: 0.000009\n",
            "Train Epoch: 72 [4864/60000 (8%)]\tLoss: 0.000008\n",
            "Train Epoch: 72 [9728/60000 (16%)]\tLoss: 0.000006\n",
            "Train Epoch: 72 [14592/60000 (24%)]\tLoss: 0.000010\n",
            "Train Epoch: 72 [19456/60000 (32%)]\tLoss: 0.000010\n",
            "Train Epoch: 72 [24320/60000 (40%)]\tLoss: 0.000005\n",
            "Train Epoch: 72 [29184/60000 (49%)]\tLoss: 0.000018\n",
            "Train Epoch: 72 [34048/60000 (57%)]\tLoss: 0.000006\n",
            "Train Epoch: 72 [38912/60000 (65%)]\tLoss: 0.000007\n",
            "Train Epoch: 72 [43776/60000 (73%)]\tLoss: 0.000027\n",
            "Train Epoch: 72 [48640/60000 (81%)]\tLoss: 0.000009\n",
            "Train Epoch: 72 [53504/60000 (89%)]\tLoss: 0.000015\n",
            "Train Epoch: 72 [58368/60000 (97%)]\tLoss: 0.000013\n",
            "\n",
            "Test set: Average loss: 0.6636, Accuracy: 9174/10000 (92%)\n",
            "\n",
            "Train Epoch: 73 [0/60000 (0%)]\tLoss: 0.000019\n",
            "Train Epoch: 73 [4864/60000 (8%)]\tLoss: 0.000007\n",
            "Train Epoch: 73 [9728/60000 (16%)]\tLoss: 0.000007\n",
            "Train Epoch: 73 [14592/60000 (24%)]\tLoss: 0.000006\n",
            "Train Epoch: 73 [19456/60000 (32%)]\tLoss: 0.000019\n",
            "Train Epoch: 73 [24320/60000 (40%)]\tLoss: 0.000008\n",
            "Train Epoch: 73 [29184/60000 (49%)]\tLoss: 0.000051\n",
            "Train Epoch: 73 [34048/60000 (57%)]\tLoss: 0.000007\n",
            "Train Epoch: 73 [38912/60000 (65%)]\tLoss: 0.000010\n",
            "Train Epoch: 73 [43776/60000 (73%)]\tLoss: 0.000006\n",
            "Train Epoch: 73 [48640/60000 (81%)]\tLoss: 0.000016\n",
            "Train Epoch: 73 [53504/60000 (89%)]\tLoss: 0.000017\n",
            "Train Epoch: 73 [58368/60000 (97%)]\tLoss: 0.000007\n",
            "\n",
            "Test set: Average loss: 0.6606, Accuracy: 9181/10000 (92%)\n",
            "\n",
            "Train Epoch: 74 [0/60000 (0%)]\tLoss: 0.000011\n",
            "Train Epoch: 74 [4864/60000 (8%)]\tLoss: 0.000006\n",
            "Train Epoch: 74 [9728/60000 (16%)]\tLoss: 0.000013\n",
            "Train Epoch: 74 [14592/60000 (24%)]\tLoss: 0.000007\n",
            "Train Epoch: 74 [19456/60000 (32%)]\tLoss: 0.000017\n",
            "Train Epoch: 74 [24320/60000 (40%)]\tLoss: 0.000025\n",
            "Train Epoch: 74 [29184/60000 (49%)]\tLoss: 0.000005\n",
            "Train Epoch: 74 [34048/60000 (57%)]\tLoss: 0.000010\n",
            "Train Epoch: 74 [38912/60000 (65%)]\tLoss: 0.000013\n",
            "Train Epoch: 74 [43776/60000 (73%)]\tLoss: 0.000007\n",
            "Train Epoch: 74 [48640/60000 (81%)]\tLoss: 0.000025\n",
            "Train Epoch: 74 [53504/60000 (89%)]\tLoss: 0.000008\n",
            "Train Epoch: 74 [58368/60000 (97%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 0.6616, Accuracy: 9179/10000 (92%)\n",
            "\n",
            "Train Epoch: 75 [0/60000 (0%)]\tLoss: 0.000004\n",
            "Train Epoch: 75 [4864/60000 (8%)]\tLoss: 0.000006\n",
            "Train Epoch: 75 [9728/60000 (16%)]\tLoss: 0.000008\n",
            "Train Epoch: 75 [14592/60000 (24%)]\tLoss: 0.000008\n",
            "Train Epoch: 75 [19456/60000 (32%)]\tLoss: 0.000011\n",
            "Train Epoch: 75 [24320/60000 (40%)]\tLoss: 0.000011\n",
            "Train Epoch: 75 [29184/60000 (49%)]\tLoss: 0.000011\n",
            "Train Epoch: 75 [34048/60000 (57%)]\tLoss: 0.000007\n",
            "Train Epoch: 75 [38912/60000 (65%)]\tLoss: 0.000005\n",
            "Train Epoch: 75 [43776/60000 (73%)]\tLoss: 0.000003\n",
            "Train Epoch: 75 [48640/60000 (81%)]\tLoss: 0.000007\n",
            "Train Epoch: 75 [53504/60000 (89%)]\tLoss: 0.000006\n",
            "Train Epoch: 75 [58368/60000 (97%)]\tLoss: 0.000007\n",
            "\n",
            "Test set: Average loss: 0.6626, Accuracy: 9181/10000 (92%)\n",
            "\n",
            "Train Epoch: 76 [0/60000 (0%)]\tLoss: 0.000011\n",
            "Train Epoch: 76 [4864/60000 (8%)]\tLoss: 0.000006\n",
            "Train Epoch: 76 [9728/60000 (16%)]\tLoss: 0.000013\n",
            "Train Epoch: 76 [14592/60000 (24%)]\tLoss: 0.000008\n",
            "Train Epoch: 76 [19456/60000 (32%)]\tLoss: 0.000010\n",
            "Train Epoch: 76 [24320/60000 (40%)]\tLoss: 0.000027\n",
            "Train Epoch: 76 [29184/60000 (49%)]\tLoss: 0.000005\n",
            "Train Epoch: 76 [34048/60000 (57%)]\tLoss: 0.000009\n",
            "Train Epoch: 76 [38912/60000 (65%)]\tLoss: 0.000018\n",
            "Train Epoch: 76 [43776/60000 (73%)]\tLoss: 0.000014\n",
            "Train Epoch: 76 [48640/60000 (81%)]\tLoss: 0.000012\n",
            "Train Epoch: 76 [53504/60000 (89%)]\tLoss: 0.000010\n",
            "Train Epoch: 76 [58368/60000 (97%)]\tLoss: 0.000009\n",
            "\n",
            "Test set: Average loss: 0.6636, Accuracy: 9179/10000 (92%)\n",
            "\n",
            "Train Epoch: 77 [0/60000 (0%)]\tLoss: 0.000008\n",
            "Train Epoch: 77 [4864/60000 (8%)]\tLoss: 0.000006\n",
            "Train Epoch: 77 [9728/60000 (16%)]\tLoss: 0.000017\n",
            "Train Epoch: 77 [14592/60000 (24%)]\tLoss: 0.000019\n",
            "Train Epoch: 77 [19456/60000 (32%)]\tLoss: 0.000010\n",
            "Train Epoch: 77 [24320/60000 (40%)]\tLoss: 0.000022\n",
            "Train Epoch: 77 [29184/60000 (49%)]\tLoss: 0.000010\n",
            "Train Epoch: 77 [34048/60000 (57%)]\tLoss: 0.000007\n",
            "Train Epoch: 77 [38912/60000 (65%)]\tLoss: 0.000008\n",
            "Train Epoch: 77 [43776/60000 (73%)]\tLoss: 0.000005\n",
            "Train Epoch: 77 [48640/60000 (81%)]\tLoss: 0.000007\n",
            "Train Epoch: 77 [53504/60000 (89%)]\tLoss: 0.000026\n",
            "Train Epoch: 77 [58368/60000 (97%)]\tLoss: 0.000006\n",
            "\n",
            "Test set: Average loss: 0.6610, Accuracy: 9180/10000 (92%)\n",
            "\n",
            "Train Epoch: 78 [0/60000 (0%)]\tLoss: 0.000011\n",
            "Train Epoch: 78 [4864/60000 (8%)]\tLoss: 0.000006\n",
            "Train Epoch: 78 [9728/60000 (16%)]\tLoss: 0.000006\n",
            "Train Epoch: 78 [14592/60000 (24%)]\tLoss: 0.000008\n",
            "Train Epoch: 78 [19456/60000 (32%)]\tLoss: 0.000007\n",
            "Train Epoch: 78 [24320/60000 (40%)]\tLoss: 0.000016\n",
            "Train Epoch: 78 [29184/60000 (49%)]\tLoss: 0.000030\n",
            "Train Epoch: 78 [34048/60000 (57%)]\tLoss: 0.000006\n",
            "Train Epoch: 78 [38912/60000 (65%)]\tLoss: 0.000006\n",
            "Train Epoch: 78 [43776/60000 (73%)]\tLoss: 0.000007\n",
            "Train Epoch: 78 [48640/60000 (81%)]\tLoss: 0.000008\n",
            "Train Epoch: 78 [53504/60000 (89%)]\tLoss: 0.000008\n",
            "Train Epoch: 78 [58368/60000 (97%)]\tLoss: 0.000007\n",
            "\n",
            "Test set: Average loss: 0.6666, Accuracy: 9181/10000 (92%)\n",
            "\n",
            "Train Epoch: 79 [0/60000 (0%)]\tLoss: 0.000014\n",
            "Train Epoch: 79 [4864/60000 (8%)]\tLoss: 0.000008\n",
            "Train Epoch: 79 [9728/60000 (16%)]\tLoss: 0.000006\n",
            "Train Epoch: 79 [14592/60000 (24%)]\tLoss: 0.000013\n",
            "Train Epoch: 79 [19456/60000 (32%)]\tLoss: 0.000013\n",
            "Train Epoch: 79 [24320/60000 (40%)]\tLoss: 0.000010\n",
            "Train Epoch: 79 [29184/60000 (49%)]\tLoss: 0.000015\n",
            "Train Epoch: 79 [34048/60000 (57%)]\tLoss: 0.000007\n",
            "Train Epoch: 79 [38912/60000 (65%)]\tLoss: 0.000010\n",
            "Train Epoch: 79 [43776/60000 (73%)]\tLoss: 0.000008\n",
            "Train Epoch: 79 [48640/60000 (81%)]\tLoss: 0.000008\n",
            "Train Epoch: 79 [53504/60000 (89%)]\tLoss: 0.000027\n",
            "Train Epoch: 79 [58368/60000 (97%)]\tLoss: 0.000007\n",
            "\n",
            "Test set: Average loss: 0.6649, Accuracy: 9182/10000 (92%)\n",
            "\n",
            "Train Epoch: 80 [0/60000 (0%)]\tLoss: 0.000067\n",
            "Train Epoch: 80 [4864/60000 (8%)]\tLoss: 0.000003\n",
            "Train Epoch: 80 [9728/60000 (16%)]\tLoss: 0.000005\n",
            "Train Epoch: 80 [14592/60000 (24%)]\tLoss: 0.000016\n",
            "Train Epoch: 80 [19456/60000 (32%)]\tLoss: 0.000006\n",
            "Train Epoch: 80 [24320/60000 (40%)]\tLoss: 0.000004\n",
            "Train Epoch: 80 [29184/60000 (49%)]\tLoss: 0.000009\n",
            "Train Epoch: 80 [34048/60000 (57%)]\tLoss: 0.000010\n",
            "Train Epoch: 80 [38912/60000 (65%)]\tLoss: 0.000011\n",
            "Train Epoch: 80 [43776/60000 (73%)]\tLoss: 0.000012\n",
            "Train Epoch: 80 [48640/60000 (81%)]\tLoss: 0.000005\n",
            "Train Epoch: 80 [53504/60000 (89%)]\tLoss: 0.000004\n",
            "Train Epoch: 80 [58368/60000 (97%)]\tLoss: 0.000009\n",
            "\n",
            "Test set: Average loss: 0.6627, Accuracy: 9177/10000 (92%)\n",
            "\n",
            "Train Epoch: 81 [0/60000 (0%)]\tLoss: 0.000005\n",
            "Train Epoch: 81 [4864/60000 (8%)]\tLoss: 0.000010\n",
            "Train Epoch: 81 [9728/60000 (16%)]\tLoss: 0.000012\n",
            "Train Epoch: 81 [14592/60000 (24%)]\tLoss: 0.000008\n",
            "Train Epoch: 81 [19456/60000 (32%)]\tLoss: 0.000036\n",
            "Train Epoch: 81 [24320/60000 (40%)]\tLoss: 0.000010\n",
            "Train Epoch: 81 [29184/60000 (49%)]\tLoss: 0.000013\n",
            "Train Epoch: 81 [34048/60000 (57%)]\tLoss: 0.000059\n",
            "Train Epoch: 81 [38912/60000 (65%)]\tLoss: 0.000002\n",
            "Train Epoch: 81 [43776/60000 (73%)]\tLoss: 0.000010\n",
            "Train Epoch: 81 [48640/60000 (81%)]\tLoss: 0.000005\n",
            "Train Epoch: 81 [53504/60000 (89%)]\tLoss: 0.000003\n",
            "Train Epoch: 81 [58368/60000 (97%)]\tLoss: 0.000010\n",
            "\n",
            "Test set: Average loss: 0.6714, Accuracy: 9176/10000 (92%)\n",
            "\n",
            "Train Epoch: 82 [0/60000 (0%)]\tLoss: 0.000008\n",
            "Train Epoch: 82 [4864/60000 (8%)]\tLoss: 0.000005\n",
            "Train Epoch: 82 [9728/60000 (16%)]\tLoss: 0.000013\n",
            "Train Epoch: 82 [14592/60000 (24%)]\tLoss: 0.000006\n",
            "Train Epoch: 82 [19456/60000 (32%)]\tLoss: 0.000007\n",
            "Train Epoch: 82 [24320/60000 (40%)]\tLoss: 0.000019\n",
            "Train Epoch: 82 [29184/60000 (49%)]\tLoss: 0.000010\n",
            "Train Epoch: 82 [34048/60000 (57%)]\tLoss: 0.000008\n",
            "Train Epoch: 82 [38912/60000 (65%)]\tLoss: 0.000004\n",
            "Train Epoch: 82 [43776/60000 (73%)]\tLoss: 0.000007\n",
            "Train Epoch: 82 [48640/60000 (81%)]\tLoss: 0.000013\n",
            "Train Epoch: 82 [53504/60000 (89%)]\tLoss: 0.000008\n",
            "Train Epoch: 82 [58368/60000 (97%)]\tLoss: 0.000008\n",
            "\n",
            "Test set: Average loss: 0.6664, Accuracy: 9178/10000 (92%)\n",
            "\n",
            "Train Epoch: 83 [0/60000 (0%)]\tLoss: 0.000003\n",
            "Train Epoch: 83 [4864/60000 (8%)]\tLoss: 0.000006\n",
            "Train Epoch: 83 [9728/60000 (16%)]\tLoss: 0.000005\n",
            "Train Epoch: 83 [14592/60000 (24%)]\tLoss: 0.000007\n",
            "Train Epoch: 83 [19456/60000 (32%)]\tLoss: 0.000009\n",
            "Train Epoch: 83 [24320/60000 (40%)]\tLoss: 0.000011\n",
            "Train Epoch: 83 [29184/60000 (49%)]\tLoss: 0.000015\n",
            "Train Epoch: 83 [34048/60000 (57%)]\tLoss: 0.000005\n",
            "Train Epoch: 83 [38912/60000 (65%)]\tLoss: 0.000008\n",
            "Train Epoch: 83 [43776/60000 (73%)]\tLoss: 0.000018\n",
            "Train Epoch: 83 [48640/60000 (81%)]\tLoss: 0.000012\n",
            "Train Epoch: 83 [53504/60000 (89%)]\tLoss: 0.000005\n",
            "Train Epoch: 83 [58368/60000 (97%)]\tLoss: 0.000102\n",
            "\n",
            "Test set: Average loss: 0.6670, Accuracy: 9182/10000 (92%)\n",
            "\n",
            "Train Epoch: 84 [0/60000 (0%)]\tLoss: 0.000007\n",
            "Train Epoch: 84 [4864/60000 (8%)]\tLoss: 0.000004\n",
            "Train Epoch: 84 [9728/60000 (16%)]\tLoss: 0.000009\n",
            "Train Epoch: 84 [14592/60000 (24%)]\tLoss: 0.000012\n",
            "Train Epoch: 84 [19456/60000 (32%)]\tLoss: 0.000011\n",
            "Train Epoch: 84 [24320/60000 (40%)]\tLoss: 0.000048\n",
            "Train Epoch: 84 [29184/60000 (49%)]\tLoss: 0.000008\n",
            "Train Epoch: 84 [34048/60000 (57%)]\tLoss: 0.000005\n",
            "Train Epoch: 84 [38912/60000 (65%)]\tLoss: 0.000050\n",
            "Train Epoch: 84 [43776/60000 (73%)]\tLoss: 0.000005\n",
            "Train Epoch: 84 [48640/60000 (81%)]\tLoss: 0.000012\n",
            "Train Epoch: 84 [53504/60000 (89%)]\tLoss: 0.000003\n",
            "Train Epoch: 84 [58368/60000 (97%)]\tLoss: 0.000010\n",
            "\n",
            "Test set: Average loss: 0.6689, Accuracy: 9185/10000 (92%)\n",
            "\n",
            "Train Epoch: 85 [0/60000 (0%)]\tLoss: 0.000017\n",
            "Train Epoch: 85 [4864/60000 (8%)]\tLoss: 0.000009\n",
            "Train Epoch: 85 [9728/60000 (16%)]\tLoss: 0.000009\n",
            "Train Epoch: 85 [14592/60000 (24%)]\tLoss: 0.000006\n",
            "Train Epoch: 85 [19456/60000 (32%)]\tLoss: 0.000007\n",
            "Train Epoch: 85 [24320/60000 (40%)]\tLoss: 0.000013\n",
            "Train Epoch: 85 [29184/60000 (49%)]\tLoss: 0.000006\n",
            "Train Epoch: 85 [34048/60000 (57%)]\tLoss: 0.000003\n",
            "Train Epoch: 85 [38912/60000 (65%)]\tLoss: 0.000023\n",
            "Train Epoch: 85 [43776/60000 (73%)]\tLoss: 0.000010\n",
            "Train Epoch: 85 [48640/60000 (81%)]\tLoss: 0.000008\n",
            "Train Epoch: 85 [53504/60000 (89%)]\tLoss: 0.000013\n",
            "Train Epoch: 85 [58368/60000 (97%)]\tLoss: 0.000008\n",
            "\n",
            "Test set: Average loss: 0.6698, Accuracy: 9181/10000 (92%)\n",
            "\n",
            "Train Epoch: 86 [0/60000 (0%)]\tLoss: 0.000008\n",
            "Train Epoch: 86 [4864/60000 (8%)]\tLoss: 0.000016\n",
            "Train Epoch: 86 [9728/60000 (16%)]\tLoss: 0.000006\n",
            "Train Epoch: 86 [14592/60000 (24%)]\tLoss: 0.000009\n",
            "Train Epoch: 86 [19456/60000 (32%)]\tLoss: 0.000013\n",
            "Train Epoch: 86 [24320/60000 (40%)]\tLoss: 0.000006\n",
            "Train Epoch: 86 [29184/60000 (49%)]\tLoss: 0.000089\n",
            "Train Epoch: 86 [34048/60000 (57%)]\tLoss: 0.000005\n",
            "Train Epoch: 86 [38912/60000 (65%)]\tLoss: 0.000006\n",
            "Train Epoch: 86 [43776/60000 (73%)]\tLoss: 0.000020\n",
            "Train Epoch: 86 [48640/60000 (81%)]\tLoss: 0.000005\n",
            "Train Epoch: 86 [53504/60000 (89%)]\tLoss: 0.000009\n",
            "Train Epoch: 86 [58368/60000 (97%)]\tLoss: 0.000007\n",
            "\n",
            "Test set: Average loss: 0.6723, Accuracy: 9188/10000 (92%)\n",
            "\n",
            "Train Epoch: 87 [0/60000 (0%)]\tLoss: 0.000008\n",
            "Train Epoch: 87 [4864/60000 (8%)]\tLoss: 0.000009\n",
            "Train Epoch: 87 [9728/60000 (16%)]\tLoss: 0.000005\n",
            "Train Epoch: 87 [14592/60000 (24%)]\tLoss: 0.000003\n",
            "Train Epoch: 87 [19456/60000 (32%)]\tLoss: 0.000006\n",
            "Train Epoch: 87 [24320/60000 (40%)]\tLoss: 0.000003\n",
            "Train Epoch: 87 [29184/60000 (49%)]\tLoss: 0.000006\n",
            "Train Epoch: 87 [34048/60000 (57%)]\tLoss: 0.000007\n",
            "Train Epoch: 87 [38912/60000 (65%)]\tLoss: 0.000004\n",
            "Train Epoch: 87 [43776/60000 (73%)]\tLoss: 0.000017\n",
            "Train Epoch: 87 [48640/60000 (81%)]\tLoss: 0.000003\n",
            "Train Epoch: 87 [53504/60000 (89%)]\tLoss: 0.000011\n",
            "Train Epoch: 87 [58368/60000 (97%)]\tLoss: 0.000013\n",
            "\n",
            "Test set: Average loss: 0.6605, Accuracy: 9185/10000 (92%)\n",
            "\n",
            "Train Epoch: 88 [0/60000 (0%)]\tLoss: 0.000019\n",
            "Train Epoch: 88 [4864/60000 (8%)]\tLoss: 0.000005\n",
            "Train Epoch: 88 [9728/60000 (16%)]\tLoss: 0.000009\n",
            "Train Epoch: 88 [14592/60000 (24%)]\tLoss: 0.000004\n",
            "Train Epoch: 88 [19456/60000 (32%)]\tLoss: 0.000007\n",
            "Train Epoch: 88 [24320/60000 (40%)]\tLoss: 0.000006\n",
            "Train Epoch: 88 [29184/60000 (49%)]\tLoss: 0.000009\n",
            "Train Epoch: 88 [34048/60000 (57%)]\tLoss: 0.000004\n",
            "Train Epoch: 88 [38912/60000 (65%)]\tLoss: 0.000008\n",
            "Train Epoch: 88 [43776/60000 (73%)]\tLoss: 0.000018\n",
            "Train Epoch: 88 [48640/60000 (81%)]\tLoss: 0.000005\n",
            "Train Epoch: 88 [53504/60000 (89%)]\tLoss: 0.000008\n",
            "Train Epoch: 88 [58368/60000 (97%)]\tLoss: 0.000006\n",
            "\n",
            "Test set: Average loss: 0.6719, Accuracy: 9179/10000 (92%)\n",
            "\n",
            "Train Epoch: 89 [0/60000 (0%)]\tLoss: 0.000006\n",
            "Train Epoch: 89 [4864/60000 (8%)]\tLoss: 0.000020\n",
            "Train Epoch: 89 [9728/60000 (16%)]\tLoss: 0.000011\n",
            "Train Epoch: 89 [14592/60000 (24%)]\tLoss: 0.000004\n",
            "Train Epoch: 89 [19456/60000 (32%)]\tLoss: 0.000015\n",
            "Train Epoch: 89 [24320/60000 (40%)]\tLoss: 0.000015\n",
            "Train Epoch: 89 [29184/60000 (49%)]\tLoss: 0.000016\n",
            "Train Epoch: 89 [34048/60000 (57%)]\tLoss: 0.000004\n",
            "Train Epoch: 89 [38912/60000 (65%)]\tLoss: 0.000011\n",
            "Train Epoch: 89 [43776/60000 (73%)]\tLoss: 0.000006\n",
            "Train Epoch: 89 [48640/60000 (81%)]\tLoss: 0.000008\n",
            "Train Epoch: 89 [53504/60000 (89%)]\tLoss: 0.000006\n",
            "Train Epoch: 89 [58368/60000 (97%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 0.6723, Accuracy: 9180/10000 (92%)\n",
            "\n",
            "Train Epoch: 90 [0/60000 (0%)]\tLoss: 0.000012\n",
            "Train Epoch: 90 [4864/60000 (8%)]\tLoss: 0.000005\n",
            "Train Epoch: 90 [9728/60000 (16%)]\tLoss: 0.000005\n",
            "Train Epoch: 90 [14592/60000 (24%)]\tLoss: 0.000003\n",
            "Train Epoch: 90 [19456/60000 (32%)]\tLoss: 0.000009\n",
            "Train Epoch: 90 [24320/60000 (40%)]\tLoss: 0.000005\n",
            "Train Epoch: 90 [29184/60000 (49%)]\tLoss: 0.000006\n",
            "Train Epoch: 90 [34048/60000 (57%)]\tLoss: 0.000008\n",
            "Train Epoch: 90 [38912/60000 (65%)]\tLoss: 0.000010\n",
            "Train Epoch: 90 [43776/60000 (73%)]\tLoss: 0.000004\n",
            "Train Epoch: 90 [48640/60000 (81%)]\tLoss: 0.000005\n",
            "Train Epoch: 90 [53504/60000 (89%)]\tLoss: 0.000007\n",
            "Train Epoch: 90 [58368/60000 (97%)]\tLoss: 0.000013\n",
            "\n",
            "Test set: Average loss: 0.6739, Accuracy: 9174/10000 (92%)\n",
            "\n",
            "Train Epoch: 91 [0/60000 (0%)]\tLoss: 0.000004\n",
            "Train Epoch: 91 [4864/60000 (8%)]\tLoss: 0.000003\n",
            "Train Epoch: 91 [9728/60000 (16%)]\tLoss: 0.000002\n",
            "Train Epoch: 91 [14592/60000 (24%)]\tLoss: 0.000006\n",
            "Train Epoch: 91 [19456/60000 (32%)]\tLoss: 0.000003\n",
            "Train Epoch: 91 [24320/60000 (40%)]\tLoss: 0.000005\n",
            "Train Epoch: 91 [29184/60000 (49%)]\tLoss: 0.000016\n",
            "Train Epoch: 91 [34048/60000 (57%)]\tLoss: 0.000005\n",
            "Train Epoch: 91 [38912/60000 (65%)]\tLoss: 0.000004\n",
            "Train Epoch: 91 [43776/60000 (73%)]\tLoss: 0.000002\n",
            "Train Epoch: 91 [48640/60000 (81%)]\tLoss: 0.000006\n",
            "Train Epoch: 91 [53504/60000 (89%)]\tLoss: 0.000005\n",
            "Train Epoch: 91 [58368/60000 (97%)]\tLoss: 0.000007\n",
            "\n",
            "Test set: Average loss: 0.6708, Accuracy: 9179/10000 (92%)\n",
            "\n",
            "Train Epoch: 92 [0/60000 (0%)]\tLoss: 0.000008\n",
            "Train Epoch: 92 [4864/60000 (8%)]\tLoss: 0.000005\n",
            "Train Epoch: 92 [9728/60000 (16%)]\tLoss: 0.000015\n",
            "Train Epoch: 92 [14592/60000 (24%)]\tLoss: 0.000009\n",
            "Train Epoch: 92 [19456/60000 (32%)]\tLoss: 0.000003\n",
            "Train Epoch: 92 [24320/60000 (40%)]\tLoss: 0.000005\n",
            "Train Epoch: 92 [29184/60000 (49%)]\tLoss: 0.000018\n",
            "Train Epoch: 92 [34048/60000 (57%)]\tLoss: 0.000007\n",
            "Train Epoch: 92 [38912/60000 (65%)]\tLoss: 0.000004\n",
            "Train Epoch: 92 [43776/60000 (73%)]\tLoss: 0.000008\n",
            "Train Epoch: 92 [48640/60000 (81%)]\tLoss: 0.000006\n",
            "Train Epoch: 92 [53504/60000 (89%)]\tLoss: 0.000017\n",
            "Train Epoch: 92 [58368/60000 (97%)]\tLoss: 0.000008\n",
            "\n",
            "Test set: Average loss: 0.6680, Accuracy: 9182/10000 (92%)\n",
            "\n",
            "Train Epoch: 93 [0/60000 (0%)]\tLoss: 0.000004\n",
            "Train Epoch: 93 [4864/60000 (8%)]\tLoss: 0.000038\n",
            "Train Epoch: 93 [9728/60000 (16%)]\tLoss: 0.000002\n",
            "Train Epoch: 93 [14592/60000 (24%)]\tLoss: 0.000007\n",
            "Train Epoch: 93 [19456/60000 (32%)]\tLoss: 0.000008\n",
            "Train Epoch: 93 [24320/60000 (40%)]\tLoss: 0.000004\n",
            "Train Epoch: 93 [29184/60000 (49%)]\tLoss: 0.000009\n",
            "Train Epoch: 93 [34048/60000 (57%)]\tLoss: 0.000006\n",
            "Train Epoch: 93 [38912/60000 (65%)]\tLoss: 0.000011\n",
            "Train Epoch: 93 [43776/60000 (73%)]\tLoss: 0.000007\n",
            "Train Epoch: 93 [48640/60000 (81%)]\tLoss: 0.000009\n",
            "Train Epoch: 93 [53504/60000 (89%)]\tLoss: 0.000006\n",
            "Train Epoch: 93 [58368/60000 (97%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 0.6770, Accuracy: 9181/10000 (92%)\n",
            "\n",
            "Train Epoch: 94 [0/60000 (0%)]\tLoss: 0.000019\n",
            "Train Epoch: 94 [4864/60000 (8%)]\tLoss: 0.000007\n",
            "Train Epoch: 94 [9728/60000 (16%)]\tLoss: 0.000003\n",
            "Train Epoch: 94 [14592/60000 (24%)]\tLoss: 0.000006\n",
            "Train Epoch: 94 [19456/60000 (32%)]\tLoss: 0.000004\n",
            "Train Epoch: 94 [24320/60000 (40%)]\tLoss: 0.000007\n",
            "Train Epoch: 94 [29184/60000 (49%)]\tLoss: 0.000004\n",
            "Train Epoch: 94 [34048/60000 (57%)]\tLoss: 0.000009\n",
            "Train Epoch: 94 [38912/60000 (65%)]\tLoss: 0.000011\n",
            "Train Epoch: 94 [43776/60000 (73%)]\tLoss: 0.000004\n",
            "Train Epoch: 94 [48640/60000 (81%)]\tLoss: 0.000008\n",
            "Train Epoch: 94 [53504/60000 (89%)]\tLoss: 0.000003\n",
            "Train Epoch: 94 [58368/60000 (97%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 0.6736, Accuracy: 9174/10000 (92%)\n",
            "\n",
            "Train Epoch: 95 [0/60000 (0%)]\tLoss: 0.000004\n",
            "Train Epoch: 95 [4864/60000 (8%)]\tLoss: 0.000009\n",
            "Train Epoch: 95 [9728/60000 (16%)]\tLoss: 0.000007\n",
            "Train Epoch: 95 [14592/60000 (24%)]\tLoss: 0.000004\n",
            "Train Epoch: 95 [19456/60000 (32%)]\tLoss: 0.000006\n",
            "Train Epoch: 95 [24320/60000 (40%)]\tLoss: 0.000010\n",
            "Train Epoch: 95 [29184/60000 (49%)]\tLoss: 0.000004\n",
            "Train Epoch: 95 [34048/60000 (57%)]\tLoss: 0.000009\n",
            "Train Epoch: 95 [38912/60000 (65%)]\tLoss: 0.000011\n",
            "Train Epoch: 95 [43776/60000 (73%)]\tLoss: 0.000011\n",
            "Train Epoch: 95 [48640/60000 (81%)]\tLoss: 0.000005\n",
            "Train Epoch: 95 [53504/60000 (89%)]\tLoss: 0.000007\n",
            "Train Epoch: 95 [58368/60000 (97%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 0.6725, Accuracy: 9177/10000 (92%)\n",
            "\n",
            "Train Epoch: 96 [0/60000 (0%)]\tLoss: 0.000004\n",
            "Train Epoch: 96 [4864/60000 (8%)]\tLoss: 0.000004\n",
            "Train Epoch: 96 [9728/60000 (16%)]\tLoss: 0.000007\n",
            "Train Epoch: 96 [14592/60000 (24%)]\tLoss: 0.000006\n",
            "Train Epoch: 96 [19456/60000 (32%)]\tLoss: 0.000015\n",
            "Train Epoch: 96 [24320/60000 (40%)]\tLoss: 0.000006\n",
            "Train Epoch: 96 [29184/60000 (49%)]\tLoss: 0.000047\n",
            "Train Epoch: 96 [34048/60000 (57%)]\tLoss: 0.000005\n",
            "Train Epoch: 96 [38912/60000 (65%)]\tLoss: 0.000008\n",
            "Train Epoch: 96 [43776/60000 (73%)]\tLoss: 0.000006\n",
            "Train Epoch: 96 [48640/60000 (81%)]\tLoss: 0.000007\n",
            "Train Epoch: 96 [53504/60000 (89%)]\tLoss: 0.000007\n",
            "Train Epoch: 96 [58368/60000 (97%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 0.6744, Accuracy: 9188/10000 (92%)\n",
            "\n",
            "Train Epoch: 97 [0/60000 (0%)]\tLoss: 0.000006\n",
            "Train Epoch: 97 [4864/60000 (8%)]\tLoss: 0.000003\n",
            "Train Epoch: 97 [9728/60000 (16%)]\tLoss: 0.000004\n",
            "Train Epoch: 97 [14592/60000 (24%)]\tLoss: 0.000013\n",
            "Train Epoch: 97 [19456/60000 (32%)]\tLoss: 0.000008\n",
            "Train Epoch: 97 [24320/60000 (40%)]\tLoss: 0.000009\n",
            "Train Epoch: 97 [29184/60000 (49%)]\tLoss: 0.000002\n",
            "Train Epoch: 97 [34048/60000 (57%)]\tLoss: 0.000006\n",
            "Train Epoch: 97 [38912/60000 (65%)]\tLoss: 0.000011\n",
            "Train Epoch: 97 [43776/60000 (73%)]\tLoss: 0.000015\n",
            "Train Epoch: 97 [48640/60000 (81%)]\tLoss: 0.000003\n",
            "Train Epoch: 97 [53504/60000 (89%)]\tLoss: 0.000009\n",
            "Train Epoch: 97 [58368/60000 (97%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 0.6759, Accuracy: 9174/10000 (92%)\n",
            "\n",
            "Train Epoch: 98 [0/60000 (0%)]\tLoss: 0.000016\n",
            "Train Epoch: 98 [4864/60000 (8%)]\tLoss: 0.000006\n",
            "Train Epoch: 98 [9728/60000 (16%)]\tLoss: 0.000010\n",
            "Train Epoch: 98 [14592/60000 (24%)]\tLoss: 0.000013\n",
            "Train Epoch: 98 [19456/60000 (32%)]\tLoss: 0.000012\n",
            "Train Epoch: 98 [24320/60000 (40%)]\tLoss: 0.000004\n",
            "Train Epoch: 98 [29184/60000 (49%)]\tLoss: 0.000007\n",
            "Train Epoch: 98 [34048/60000 (57%)]\tLoss: 0.000008\n",
            "Train Epoch: 98 [38912/60000 (65%)]\tLoss: 0.000013\n",
            "Train Epoch: 98 [43776/60000 (73%)]\tLoss: 0.000006\n",
            "Train Epoch: 98 [48640/60000 (81%)]\tLoss: 0.000005\n",
            "Train Epoch: 98 [53504/60000 (89%)]\tLoss: 0.000004\n",
            "Train Epoch: 98 [58368/60000 (97%)]\tLoss: 0.000031\n",
            "\n",
            "Test set: Average loss: 0.6706, Accuracy: 9183/10000 (92%)\n",
            "\n",
            "Train Epoch: 99 [0/60000 (0%)]\tLoss: 0.000009\n",
            "Train Epoch: 99 [4864/60000 (8%)]\tLoss: 0.000004\n",
            "Train Epoch: 99 [9728/60000 (16%)]\tLoss: 0.000005\n",
            "Train Epoch: 99 [14592/60000 (24%)]\tLoss: 0.000009\n",
            "Train Epoch: 99 [19456/60000 (32%)]\tLoss: 0.000008\n",
            "Train Epoch: 99 [24320/60000 (40%)]\tLoss: 0.000003\n",
            "Train Epoch: 99 [29184/60000 (49%)]\tLoss: 0.000007\n",
            "Train Epoch: 99 [34048/60000 (57%)]\tLoss: 0.000009\n",
            "Train Epoch: 99 [38912/60000 (65%)]\tLoss: 0.000018\n",
            "Train Epoch: 99 [43776/60000 (73%)]\tLoss: 0.000023\n",
            "Train Epoch: 99 [48640/60000 (81%)]\tLoss: 0.000003\n",
            "Train Epoch: 99 [53504/60000 (89%)]\tLoss: 0.000008\n",
            "Train Epoch: 99 [58368/60000 (97%)]\tLoss: 0.000016\n",
            "\n",
            "Test set: Average loss: 0.6791, Accuracy: 9178/10000 (92%)\n",
            "\n",
            "Train Epoch: 100 [0/60000 (0%)]\tLoss: 0.000006\n",
            "Train Epoch: 100 [4864/60000 (8%)]\tLoss: 0.000008\n",
            "Train Epoch: 100 [9728/60000 (16%)]\tLoss: 0.000007\n",
            "Train Epoch: 100 [14592/60000 (24%)]\tLoss: 0.000009\n",
            "Train Epoch: 100 [19456/60000 (32%)]\tLoss: 0.000006\n",
            "Train Epoch: 100 [24320/60000 (40%)]\tLoss: 0.000004\n",
            "Train Epoch: 100 [29184/60000 (49%)]\tLoss: 0.000003\n",
            "Train Epoch: 100 [34048/60000 (57%)]\tLoss: 0.000005\n",
            "Train Epoch: 100 [38912/60000 (65%)]\tLoss: 0.000004\n",
            "Train Epoch: 100 [43776/60000 (73%)]\tLoss: 0.000014\n",
            "Train Epoch: 100 [48640/60000 (81%)]\tLoss: 0.000008\n",
            "Train Epoch: 100 [53504/60000 (89%)]\tLoss: 0.000004\n",
            "Train Epoch: 100 [58368/60000 (97%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 0.6723, Accuracy: 9182/10000 (92%)\n",
            "\n",
            "ResNet(\n",
            "  11.18 M, 100.000% Params, 1.74 GMac, 100.000% MACs, \n",
            "  (conv1): Conv2d(3.14 k, 0.028% Params, 39.34 MMac, 2.257% MACs, 1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.092% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(0, 0.000% Params, 802.82 KMac, 0.046% MACs, inplace=True)\n",
            "  (maxpool): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.046% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    147.97 k, 1.324% Params, 464.83 MMac, 26.669% MACs, \n",
            "    (0): BasicBlock(\n",
            "      73.98 k, 0.662% Params, 232.42 MMac, 13.334% MACs, \n",
            "      (conv1): Conv2d(36.86 k, 0.330% Params, 115.61 MMac, 6.633% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 401.41 KMac, 0.023% MACs, inplace=True)\n",
            "      (conv2): Conv2d(36.86 k, 0.330% Params, 115.61 MMac, 6.633% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      73.98 k, 0.662% Params, 232.42 MMac, 13.334% MACs, \n",
            "      (conv1): Conv2d(36.86 k, 0.330% Params, 115.61 MMac, 6.633% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 401.41 KMac, 0.023% MACs, inplace=True)\n",
            "      (conv2): Conv2d(36.86 k, 0.330% Params, 115.61 MMac, 6.633% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    525.57 k, 4.703% Params, 412.45 MMac, 23.663% MACs, \n",
            "    (0): BasicBlock(\n",
            "      230.14 k, 2.059% Params, 180.63 MMac, 10.363% MACs, \n",
            "      (conv1): Conv2d(73.73 k, 0.660% Params, 57.8 MMac, 3.316% MACs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, 0.002% Params, 200.7 KMac, 0.012% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 200.7 KMac, 0.012% MACs, inplace=True)\n",
            "      (conv2): Conv2d(147.46 k, 1.319% Params, 115.61 MMac, 6.633% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, 0.002% Params, 200.7 KMac, 0.012% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        8.45 k, 0.076% Params, 6.62 MMac, 0.380% MACs, \n",
            "        (0): Conv2d(8.19 k, 0.073% Params, 6.42 MMac, 0.368% MACs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, 0.002% Params, 200.7 KMac, 0.012% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      295.42 k, 2.644% Params, 231.81 MMac, 13.300% MACs, \n",
            "      (conv1): Conv2d(147.46 k, 1.319% Params, 115.61 MMac, 6.633% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, 0.002% Params, 200.7 KMac, 0.012% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 200.7 KMac, 0.012% MACs, inplace=True)\n",
            "      (conv2): Conv2d(147.46 k, 1.319% Params, 115.61 MMac, 6.633% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, 0.002% Params, 200.7 KMac, 0.012% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    2.1 M, 18.789% Params, 411.74 MMac, 23.623% MACs, \n",
            "    (0): BasicBlock(\n",
            "      919.04 k, 8.224% Params, 180.23 MMac, 10.340% MACs, \n",
            "      (conv1): Conv2d(294.91 k, 2.639% Params, 57.8 MMac, 3.316% MACs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, 0.005% Params, 100.35 KMac, 0.006% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 100.35 KMac, 0.006% MACs, inplace=True)\n",
            "      (conv2): Conv2d(589.82 k, 5.278% Params, 115.61 MMac, 6.633% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, 0.005% Params, 100.35 KMac, 0.006% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        33.28 k, 0.298% Params, 6.52 MMac, 0.374% MACs, \n",
            "        (0): Conv2d(32.77 k, 0.293% Params, 6.42 MMac, 0.368% MACs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, 0.005% Params, 100.35 KMac, 0.006% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      1.18 M, 10.565% Params, 231.51 MMac, 13.282% MACs, \n",
            "      (conv1): Conv2d(589.82 k, 5.278% Params, 115.61 MMac, 6.633% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, 0.005% Params, 100.35 KMac, 0.006% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 100.35 KMac, 0.006% MACs, inplace=True)\n",
            "      (conv2): Conv2d(589.82 k, 5.278% Params, 115.61 MMac, 6.633% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, 0.005% Params, 100.35 KMac, 0.006% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    8.39 M, 75.109% Params, 411.39 MMac, 23.603% MACs, \n",
            "    (0): BasicBlock(\n",
            "      3.67 M, 32.868% Params, 180.03 MMac, 10.329% MACs, \n",
            "      (conv1): Conv2d(1.18 M, 10.556% Params, 57.8 MMac, 3.316% MACs, 256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1.02 k, 0.009% Params, 50.18 KMac, 0.003% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 50.18 KMac, 0.003% MACs, inplace=True)\n",
            "      (conv2): Conv2d(2.36 M, 21.112% Params, 115.61 MMac, 6.633% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1.02 k, 0.009% Params, 50.18 KMac, 0.003% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        132.1 k, 1.182% Params, 6.47 MMac, 0.371% MACs, \n",
            "        (0): Conv2d(131.07 k, 1.173% Params, 6.42 MMac, 0.368% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.009% Params, 50.18 KMac, 0.003% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      4.72 M, 42.241% Params, 231.36 MMac, 13.274% MACs, \n",
            "      (conv1): Conv2d(2.36 M, 21.112% Params, 115.61 MMac, 6.633% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(1.02 k, 0.009% Params, 50.18 KMac, 0.003% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 50.18 KMac, 0.003% MACs, inplace=True)\n",
            "      (conv2): Conv2d(2.36 M, 21.112% Params, 115.61 MMac, 6.633% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(1.02 k, 0.009% Params, 50.18 KMac, 0.003% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (averpool): AdaptiveAvgPool2d(0, 0.000% Params, 25.09 KMac, 0.001% MACs, output_size=(1, 1))\n",
            "  (fc): Linear(5.13 k, 0.046% Params, 5.13 KMac, 0.000% MACs, in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "FLOPs: 1.74 GMac\n",
            "Parameters: 11.18 M\n",
            "[tensor(87.3800), tensor(89.2100), tensor(89.5200), tensor(89.7200), tensor(90.2900), tensor(89.6200), tensor(90.0500), tensor(90.2700), tensor(90.7200), tensor(90.5600), tensor(90.7100), tensor(90.0100), tensor(90.7700), tensor(90.3200), tensor(89.8100), tensor(90.3900), tensor(90.5500), tensor(90.5700), tensor(90.6500), tensor(90.8700), tensor(90.7300), tensor(90.3100), tensor(90.5000), tensor(90.9300), tensor(90.9800), tensor(90.9900), tensor(91.0700), tensor(90.8000), tensor(91.4300), tensor(91.3700), tensor(91.0600), tensor(91.2400), tensor(91.3900), tensor(91.1200), tensor(90.5900), tensor(91.1300), tensor(91.1700), tensor(91.1000), tensor(91.6200), tensor(91.0600), tensor(91.2400), tensor(91.3100), tensor(91.1200), tensor(91.2100), tensor(91.1800), tensor(90.9700), tensor(91.4100), tensor(91.5300), tensor(91.4600), tensor(91.4600), tensor(91.6700), tensor(91.4600), tensor(91.5000), tensor(91.6900), tensor(91.6100), tensor(91.8200), tensor(91.8100), tensor(91.7200), tensor(91.7700), tensor(91.7100), tensor(91.7400), tensor(91.7500), tensor(91.8600), tensor(91.8000), tensor(91.8100), tensor(91.8400), tensor(91.8300), tensor(91.7600), tensor(91.7600), tensor(91.6800), tensor(91.8600), tensor(91.7400), tensor(91.8100), tensor(91.7900), tensor(91.8100), tensor(91.7900), tensor(91.8000), tensor(91.8100), tensor(91.8200), tensor(91.7700), tensor(91.7600), tensor(91.7800), tensor(91.8200), tensor(91.8500), tensor(91.8100), tensor(91.8800), tensor(91.8500), tensor(91.7900), tensor(91.8000), tensor(91.7400), tensor(91.7900), tensor(91.8200), tensor(91.8100), tensor(91.7400), tensor(91.7700), tensor(91.8800), tensor(91.7400), tensor(91.8300), tensor(91.7800), tensor(91.8200)]\n",
            "[tensor(2.0150), tensor(2.0517), tensor(2.0817), tensor(2.1150), tensor(2.1433), tensor(2.1333), tensor(2.1467), tensor(2.1900), tensor(2.1717), tensor(2.1733), tensor(2.1700), tensor(2.1967), tensor(2.2133), tensor(2.2233), tensor(2.2317), tensor(2.2567), tensor(2.2533), tensor(2.2400), tensor(2.2600), tensor(2.2650), tensor(2.2567), tensor(2.2617), tensor(2.2633), tensor(2.2850), tensor(2.2650), tensor(2.2733), tensor(2.2667), tensor(2.2717), tensor(2.2817), tensor(2.2783), tensor(2.2800), tensor(2.2833), tensor(2.2833), tensor(2.2750), tensor(2.2817), tensor(2.2833), tensor(2.2850), tensor(2.2917), tensor(2.2917), tensor(2.2917), tensor(2.2850), tensor(2.2917), tensor(2.2900), tensor(2.2867), tensor(2.2850), tensor(2.2917), tensor(2.2883), tensor(2.2917), tensor(2.2933), tensor(2.2917), tensor(2.2900), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933), tensor(2.2933)]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ResNeXt\n",
        "model = ResNet(Bottleneck,[2,2,2,2],groups=32)\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'],weight_decay=args['weight_decay'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "with torch.cuda.device(0):\n",
        "\n",
        "  input_tensor = torch.randn(1, 1, 224, 224).cuda()\n",
        "\n",
        "  flops, params = get_model_complexity_info(model, (1, 224, 224), as_strings=True, print_per_layer_stat=True)\n",
        "\n",
        "  print('FLOPs:', flops)\n",
        "\n",
        "  print('Parameters:', params)\n",
        "\n",
        "print(test_acc)\n",
        "print(train_acc)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot([i for i in range(1,len(test_acc)+1)], test_acc, color =\"blue\")\n",
        "a = plt.twinx()\n",
        "a.plot([i for i in range(1,len(train_acc)+1)], train_acc, color = \"red\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IL_sKr34slks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}